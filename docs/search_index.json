[
["index.html", "Best Practices for Using eBird Data Welcome", " Best Practices for Using eBird Data Matthew Strimas-Mackey, Wesley M. Hochachka, Viviana Ruiz-Gutierrez, Orin J. Robinson, Eliot T. Miller, Tom Auer, Steve Kelling, Daniel Fink, Alison Johnston Version 1.0 Welcome Best Practices for Using eBird Data is a supplement to Best practices for making reliable inferences from citizen science data: case study using eBird to estimate species distributions (Johnston et al. 2019). This paper describes the challenges associated with making inferences from biological citizen science data and proposes a set of best practices for making reliable estimates of species distributions from these data. Throughout, the paper uses eBird, the world’s largest biological citizen science project, as a case study to illustrate the good practices. This book acts as a supplement to the paper, showing readers how to implement these best practices within R using real data from eBird. After completing this book, readers should be able to extract data from the eBird database suitable for their own studies, process these data to prepare them for robust analyses, collect environmental covariates for modeling, and fit and assess models estimating encounter rate, occupancy, and relative abundance. Readers should be comfortable with the R programming language, and read the Prerequisites and Setup sections of the introduction, before diving into this book. A preprint of the paper associated with this book is currently available on bioRxiv. This book is a living document that will be regularly updated. To submit fixes or suggest additions and improvements to the book, please file an issue on GitHub. Please cite this book as: Strimas-Mackey, M., W.M. Hochachka, V. Ruiz-Gutierrez, O.J. Robinson, E.T. Miller, T. Auer, S. Kelling, D. Fink, A. Johnston. 2020. Best Practices for Using eBird Data. Version 1.0. https://cornelllabofornithology.github.io/ebird-best-practices/. Cornell Lab of Ornithology, Ithaca, New York. https://doi.org/10.5281/zenodo.3620739 "],
["intro.html", "Chapter 1 Introduction and Setup 1.1 Introduction 1.2 Prerequisites 1.3 Setup", " Chapter 1 Introduction and Setup 1.1 Introduction Citizen science data are increasingly making important contributions to ecological research and conservation. One of the most common forms of citizen science data is derived from members of the public recording species observations. eBird (Sullivan et al. 2014) is the largest of these biological citizen science programs. As of January 2019, the database contained nearly 600 million bird observations from every country in the world, with observations of nearly every bird species on Earth. The eBird database is valuable to researchers across the globe, due to its year-round, broad spatial coverage, high volumes of open access data, and applications to many ecological questions. These data have been widely used in scientific research to study phenology, species distributions, population trends, evolution, behavior, global change, and conservation. However, robust inference with eBird data requires careful processing of the data to address the challenges associated with citizen science datasets. This book, and the associated paper, outlines a set of best practices for addressing these challenges and making reliable estimates of species distributions from eBird data. There are two key characteristics that distinguish eBird from many other citizen science projects and facilitate robust ecological analyses: the checklist structure enables non-detection to be inferred and the effort information associated with a checklist facilitates robust analyses by accounting for variation in the observation process (La Sorte et al. 2018; Kelling et al. 2018). When a participant submits data to eBird, sightings of multiple species from the same observation period are grouped together into a single checklist. Complete checklists are those for which the participant reported all birds that they were able to detect and identify. Critically, this enables scientists to infer counts of zero individuals for the species that were not reported. If checklists are not complete, it’s not possible to ascertain whether the absence of a species on a list was a non-detection or the result of a participant not recording the species. In addition, citizen science projects occur on a spectrum from those with predefined sampling structures that resemble more traditional survey designs, to those that are unstructured and collect observations opportunistically. eBird is a semi-structured project, having flexible, easy to follow protocols that attract many participants, but also collecting data on the observation process (e.g. amount of time spent birding, number of observers, etc.), which can be used in subsequent analyses (Kelling et al. 2018). Despite the strengths of eBird data, species observations collected through citizen science projects present a number of challenges that are not found in conventional scientific data. The following are some of the primary challenges associated these data; challenges that will be addressed throughout this book: Taxonomic bias: participants often have preferences for certain species, which may lead to preferential recording of some species over others (Greenwood 2007; Tulloch and Szabo 2012). Restricting analyses to complete checklists largely mitigates this issue. Spatial bias: most participants in citizen science surveys sample near their homes (Luck et al. 2004), in easily accessible areas such as roadsides (Kadmon, Farber, and Danin 2004), or in areas and habitats of known high biodiversity (Prendergast et al. 1993). A simple method to reduce the spatial bias that we describe is to create an equal area grid over the region of interest, and sample a given number of checklists from within each grid cell. Temporal bias: participants preferentially sample when they are available, such as weekends (Courter et al. 2013), and at times of year when they expect to observe more birds, notably during spring migration (Sullivan et al. 2014). To address the weekend bias, we recommend using a temporal scale of a week or multiple weeks for most analyses. Spatial precision: the spatial location of an eBird checklist is given as a single latitude-longitude point; however, this may not be precise for two main reasons. First, for traveling checklists, this location represents just one point on the journey. Second, eBird checklists are often assigned to a hotspot (a common location for all birders visiting a popular birding site) rather than their true location. For these reasons, it’s not appropriate to align the eBird locations with very precise habitat covariates, and we recommend summarizing covariates within a neighborhood around the checklist location. Class imbalance: bird species that are rare or hard to detect may have data with high class imbalance, with many more checklists with non-detections than detections. For these species, a distribution model predicting that the species is absent everywhere will have high accuracy, but no ecological value. We’ll follow the methods for addressing class imbalance proposed by Robinson et al. (2018). Variation in detectability: detectability describes the probability of a species that is present in an area being detected and identified. Detectability varies by season, habitat, and species (Johnston et al. 2014, 2018). Furthermore, eBird data are collected with high variation in effort, time of day, number of observers, and external conditions such as weather, all of which can affect the detectability of species (Ellis and Taylor 2018; Oliveira et al. 2018). Therefore, detectability is particularly important to consider when comparing between seasons, habitats or species. Since eBird uses a semi-structured protocol, that collects variables associated with variation in detectability, we’ll be able to account for a larger proportion of this variation in our analyses. The remainder of this book will demonstrate how to address these challenges using real data from eBird to produce reliable estimates of species distributions. In general, we’ll take a two-pronged approach to dealing with unstructured data and maximizing the value of citizen science data: imposing more structure onto the data via data filtering and including covariates in models to account for the remaining variation. The next two chapters show how to access and prepare eBird data and land cover covariates, respectively. The remaining three chapters provide examples of different species distribution models that can be fit using these data: encounter rate models, occupancy models, and abundance models. Although these examples focus on the use of eBird data, in many cases they also apply to similar citizen science datasets. 1.2 Prerequisites To understand the code examples used throughout this book, some knowledge of the programming language R is required. If you don’t meet this requirement, or begin to feel lost trying to understand the code used in this book, we suggest consulting one of the excellent free resources available online for learning R. For those with little or no prior programming experience, Hands-On Programming with R is an excellent introduction. For those with some familiarity with the basics of R that want to take their skills to the next level, we suggest R for Data Science as the best resource for learning how to work with data within R. 1.3 Setup 1.3.1 Data package The first two chapters of this book focus on obtaining and preparing eBird and land cover data for the modeling that will occur in the remaining chapters. These steps can be time consuming and laborious. If you’d like to skip straight to the analysis, download this package of prepared data. Unzip this file so that the contents are in the data/ subdirectory of your RStudio project folder. This will allow you to jump right in to the modeling and ensure that you’re using exactly the same data as was used when creating this book. This is a good option if you don’t have enough hard drive space to store the full eBird data set, which is more than 200GB, or don’t have a fast enough internet connection to download it. 1.3.2 Software The examples throughout this website use the programming language R (R Core Team 2018) to work with eBird data. If you don’t have R installed, download it now, if you already have R, chances are you’re using an outdated version, so update it to the latest version now. R is updated regularly, and it is important that you have the most recent version of R to avoid headaches when installing packages. We suggest checking every couple months to see if a new version has been released. We strongly encourage R users to use RStudio. RStudio is not required to follow along with this book; however, it will make your R experience significantly better. If you don’t have RStudio, download it now, if you already have it, update it because new versions with useful additional features are regularly released. Pro tip: immediately go into RStudio preferences (Tools &gt; Global Options) and on the General pane uncheck “Restore .RData into workspace at startup” and set “Save workspace to .RData on exit” to “Never”. This will avoid cluttering your R session with old data and save you headaches down the road. Due to the massive size of the eBird dataset, working with it requires the Unix command-line utility AWK. You won’t need to use AWK directly, since the R package auk does this hard work for you, but you do need AWK to be installed on your computer. Linux and Mac users should already have AWK installed on their machines; however, Windows user will need to install Cygwin to gain access to AWK. Cygwin is free software that allows Windows users to use Unix tools. Cygwin should be installed in the default location (C:/cygwin/bin/gawk.exe or C:/cygwin64/bin/gawk.exe) in order for everything to work correctly. Note: there’s no need to do anything at the “Select Utilities” screen, AWK will be installed by default. 1.3.3 R packages The examples in this book use a variety of R packages for accessing eBird data, working with spatial data, data processing and manipulation, and model fitting. To install all the packages necessary to work through this book, run the following code: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;mstrimas/ebppackages&quot;) Note that several of the spatial packages require dependencies. If installing these packages fails, consult the instructions for installing dependencies on the sf package website. Finally, ensure all R packages are updated to their most recent version by clicking on the Update button on the Packages tab in RStudio. 1.3.4 Tidyverse Throughout this book, we use packages from the Tidyverse, an opinionated collection of R packages designed for data science. Packages such as ggplot2, for data visualization, and dplyr, for data manipulation, are two of the most well known Tidyverse packages; however, there are many more. In the following chapters, we often use Tidyverse functions without explanation. If you encounter a function you’re unfamiliar with, consult the documentation for help (e.g. ?mutate to see help for the dplyr function mutate()). More generally, the free online book R for Data Science by Hadley Wickham is the best introduction to working with data in R using the Tidyverse. The one piece of the Tidyverse that we will cover here, because it is ubiquitous throughout this book and unfamiliar to many, is the pipe operator %&gt;%. The pipe operator takes the expression to the left of it and “pipes” it into the first argument of the expression on the right, i.e. one can replace f(x) with x %&gt;% f(). The pipe makes code significantly more readable by avoiding nested function calls, reducing the need for intermediate variables, and making sequential operations read left-to-right. For example, to add a new variable to a data frame, then summarize using a grouping variable, the following are equivalent: library(dplyr) # pipes mtcars %&gt;% mutate(wt_kg = 454 * wt) %&gt;% group_by(cyl) %&gt;% summarize(wt_kg = mean(wt_kg)) #&gt; # A tibble: 3 x 2 #&gt; cyl wt_kg #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 1038. #&gt; 2 6 1415. #&gt; 3 8 1816. # intermediate variables mtcars_kg &lt;- mutate(mtcars, wt_kg = 454 * wt) mtcars_grouped &lt;- group_by(mtcars_kg, cyl) summarize(mtcars_grouped, wt_kg = mean(wt_kg)) #&gt; # A tibble: 3 x 2 #&gt; cyl wt_kg #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 1038. #&gt; 2 6 1415. #&gt; 3 8 1816. # nested function calls summarize( group_by( mutate(mtcars, wt_kg = 454 * wt), cyl ), wt_kg = mean(wt_kg) ) #&gt; # A tibble: 3 x 2 #&gt; cyl wt_kg #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 1038. #&gt; 2 6 1415. #&gt; 3 8 1816. Once you become familiar with the pipe operator, we believe you’ll find the the above example using the pipe the easiest of the three to read and interpret. 1.3.5 Getting eBird data access The complete eBird database is provided via the eBird Basic Dataset (EBD), a large text file. To access the EBD, begin by creating an eBird account and signing in. Then visit the eBird Data Access page and fill out the data access request form. eBird data access is free; however, you will need to request access in order to download the EBD. Filling out the access request form allows eBird to keep track of the number of people using the data and obtain information on the applications for which the data are used. Once you have access to the data, proceed to the download page. Download both the World EBD (~ 42 GB compressed, ~ 210 GB uncompressed) and corresponding Sampling Event Data (~ 3.5 GB compressed, ~ 11 GB uncompressed). The former provides observation-level data, while the latter provides checklist-level data; both files are required for species distribution modeling. If limited hard drive space or a slow internet connection make dealing with these large files challenging, consult Section 2.6.1 for details on a method for downloading a subset of EBD. The downloaded data files will be in .tar format, and should be unarchived. The resulting directories will contain files with extension .txt.gz, these files should be uncompressed (on Windows use 7-Zip, on Mac use the default system uncompression utility) to produce two text files (e.g., ebd_relAug-2019.txt and ebd_sampling_relAug-2019.txt). Move these two large, uncompressed .txt files to a sensible, central location on your computer. In general, we suggest creating an ebird/ folder nested in a data/ folder within your home directory (i.e. ~/data/ebird/) to store these files, and throughout the remainder of this chapter we’ll assume you’ve placed the data there. If you choose to store the EBD elsewhere, you will need to update references to this folder in the code. If the files are too large to fit on your computer’s hard drive, they can be stored on an external hard drive. Each time you want to access eBird data in an R project, you’ll need to reference the full path to these text files, for example ~/data/ebird/ebd_relAug-2019.txt. In general, it’s best to avoid using absolute paths in R scripts because it makes them less portable–if you’re sharing the files with someone else, they’ll need to change the file paths to point to the location at which they’ve stored the eBird data. The R package auk provides a workaround for this, by allowing users to set an environment variable (EBD_PATH) that points to the directory where you’ve stored the eBird data. To set this variable, use the function auk_set_ebd_path(). For example, if the EBD and Sampling Event Data files are in ~/data/ebird/, use: # set ebd path auk::auk_set_ebd_path(&quot;~/data/ebird/&quot;) After restarting your R session, you should be able to refer directly to the EBD or Sampling Event Data files within auk functions (e.g., auk_ebd(&quot;ebd_relAug-2019.txt&quot;)). Provided your collaborators have also set EDB_PATH, your scripts should now be portable. You now have access to the full eBird dataset! Note, however, that the EBD is updated monthly. If you want the most recent eBird records, be sure to regularly download an updated version. Finally, whenever you update the EBD, always update the auk package as well, this will ensure that auk will be able to handle any changes to the EBD that may have occurred. 1.3.6 GIS data Throughout this book, we’ll be producing maps of species distributions. To provide context for these distributions, we’ll need GIS data for political boundaries. Natural Earth is the best source for a range of tightly integrated vector and raster GIS data for producing professional cartographic maps. The R package, rnaturalearth provides a convenient method for accessing these data from within R. We’ll also need Bird Conservation Region (BCR) boundaries, which are available through Bird Studies Canada. These GIS data layers are most easily accessed by downloading the data package for this book. These data were generated using the following code. If you intend to run this code yourself, first create an RStudio project, so the files will be stored within the data/ subdirectory of the project. This will allow us to load these data in later chapters as they’re needed. Note that calls to ne_download() often produce warnings suggesting that you’ve used the incorrect “category”; these can safely be ignored. library(sf) library(rnaturalearth) library(dplyr) # file to save spatial data gpkg_dir &lt;- &quot;data&quot; if (!dir.exists(gpkg_dir)) { dir.create(gpkg_dir) } f_ne &lt;- file.path(gpkg_dir, &quot;gis-data.gpkg&quot;) # download bcrs tmp_dir &lt;- normalizePath(tempdir()) tmp_bcr &lt;- file.path(tmp_dir, &quot;bcr.zip&quot;) paste0(&quot;https://www.birdscanada.org/research/gislab/download/&quot;, &quot;bcr_terrestrial_shape.zip&quot;) %&gt;% download.file(destfile = tmp_bcr) unzip(tmp_bcr, exdir = tmp_dir) bcr &lt;- file.path(tmp_dir, &quot;BCR_Terrestrial_master_International.shp&quot;) %&gt;% read_sf() %&gt;% select(bcr_code = BCR, bcr_name = LABEL) %&gt;% filter(bcr_code == 27) # clean up list.files(tmp_dir, &quot;bcr&quot;, ignore.case = TRUE, full.names = TRUE) %&gt;% unlink() # political boundaries # land border with lakes removed ne_land &lt;- ne_download(scale = 50, category = &quot;cultural&quot;, type = &quot;admin_0_countries_lakes&quot;, returnclass = &quot;sf&quot;) %&gt;% filter(CONTINENT == &quot;North America&quot;) %&gt;% st_set_precision(1e6) %&gt;% st_union() # country lines # downloaded globally then filtered to north america with st_intersect() ne_country_lines &lt;- ne_download(scale = 50, category = &quot;cultural&quot;, type = &quot;admin_0_boundary_lines_land&quot;, returnclass = &quot;sf&quot;) %&gt;% st_geometry() ne_country_lines &lt;- st_intersects(ne_country_lines, ne_land, sparse = FALSE) %&gt;% as.logical() %&gt;% {ne_country_lines[.]} # states, north america ne_state_lines &lt;- ne_download(scale = 50, category = &quot;cultural&quot;, type = &quot;admin_1_states_provinces_lines&quot;, returnclass = &quot;sf&quot;) %&gt;% filter(adm0_a3 %in% c(&quot;USA&quot;, &quot;CAN&quot;)) %&gt;% mutate(iso_a2 = recode(adm0_a3, USA = &quot;US&quot;, CAN = &quot;CAN&quot;)) %&gt;% select(country = adm0_name, country_code = iso_a2) # output unlink(f_ne) write_sf(ne_land, f_ne, &quot;ne_land&quot;) write_sf(ne_country_lines, f_ne, &quot;ne_country_lines&quot;) write_sf(ne_state_lines, f_ne, &quot;ne_state_lines&quot;) write_sf(bcr, f_ne, &quot;bcr&quot;) References "],
["ebird.html", "Chapter 2 eBird Data 2.1 Introduction 2.2 Data extraction with auk 2.3 Importing and zero-filling 2.4 Accounting for variation in detectability 2.5 Exploratory analysis and visualization 2.6 EBD file size issues", " Chapter 2 eBird Data 2.1 Introduction eBird data are collected and organized around the concept of a checklist, representing observations from a single birding event, such as a 1 km walk through a park or 15 minutes observing bird feeders in your backyard. Each checklist contains a list of species observed, counts of the number of individuals seen of each species, the location and time of the observations, and a measure of the effort expended while collecting these data. The following image depicts a typical eBird checklist as viewed on the eBird website: Although eBird collects semi-structured citizen science data, three elements of eBird checklists distinguish them from similar sources. First, eBird checklist require users to specify the survey protocol they used, whether it’s traveling, stationary, incidental (i.e. if the observations were collected when birding was not the primary activity), or one of the other protocols. Second, in addition to typical information on when and where the data were collected, checklists contain effort information specifying how long the observer searched, how far they traveled, and how many observers were part of the party. Finally, observers are asked to indicate whether they are reporting all the birds they were able to identify. Checklists with all species reported, known as complete checklists, enable researchers to identify which species were not detected (rather than just not reported). These inferred non-detections allow data to be zero-filled, so there’s a zero count for any species not recorded. Complete checklists with effort information facilitate robust analyses, and thus represent the gold standard of eBird checklists. Because of these factors, eBird data are often referred to as semi-structured (Kelling et al. 2018). Access to the complete set of eBird observations is provided via the eBird Basic Dataset (EBD). This is a tab-separated text file, released monthly, containing all validated bird sightings in the eBird database at the time of release. Each row corresponds to the sighting of a single species within a checklist and, in addition to the species and number of individuals reported, information is provided about the checklist (location, time, date, search effort, etc.). An additional file, the Sampling Event Data (SED), provides just the checklist data. In this file, each row corresponds to a checklist and only the checklist variables are included, not the associated species data. For complete checklists, the SED provides the full set of checklists in the database, which is needed to zero-fill the data. In particular, if a checklist appears in the SED as complete, but has no records in the EBD for a given species, we can infer that there is a 0 count for that species on that checklist. In addition, checklists with no species recorded (yes, these do exist), will not appear in the EBD, so the SED is required to identify these checklists. In the previous chapter, we described how to access and download the EBD. In this chapter, we’ll demonstrate how to use the R package auk to extract subsets of the data for analysis. Next, we’ll show how to import the data into R and perform some pre-processing steps required to ensure proper analysis of the data. Finally, we’ll zero-fill the data to produce presence-absence eBird data suitable for modeling species distribution and abundance. In the interest of making examples concrete, throughout this chapter, and those that follow, we’ll use the specific example of Wood Thrush in Bird Conservation Region (BCR) 27 (“Southeastern Coastal Plains”) in June for our analyses. Before we get started, we suggest creating a new RStudio project for following along with these examples; this will ensure your working directory is always the project directory and allow us to use relative paths. 2.1.1 eBird taxonomy eBird uses its own taxonomic system, updated annually after expert review every August. A notable benefit of these annual, system-wide updates is that splits and lumps are nearly instantly propagated across the entire eBird database. When necessary, expert reviewers manually sort through observations and assign records to current taxonomic hypotheses to the extent possible. This ensures that both new and historical eBird data conform to the updated taxonomy. In addition, after the taxonomy update, both the EBD and auk are updated to reflect the changes. We emphasize that for any eBird analysis, you should consider the taxonomy of your study organisms, and whether you’re accessing the intended data from eBird. For example, are you interested in all Willets or do you want to treat the eastern and western populations separately since they’re completely allopatric on their breeding grounds? Do you want to do an analysis combining Greater and Lesser Yellowlegs? eBirders have the option of recording observations for taxa above (e.g. at genus level) or below (e.g. at subspecies level) the species level. These more granular taxonomic assignments are available in the EBD. In many cases taxonomic differences can be dealt with by querying specific subspecies, or multiple species, and splitting and lumping after the query as necessary. However, note that the majority of eBird users don’t assign their observations to subspecies, so querying taxa below species will result in a much smaller dataset. If you’re not interested in these taxonomic nuances, auk will handle the taxonomy seamlessly for you by resolving all observations to species level. Indeed, for the example in this chapter, we’ll use all Wood Thrush observations and not delve into taxonomic issues further. 2.2 Data extraction with auk eBird contains an impressive amount of data (over 600 million bird observations!); however, this makes the EBD particularly challenging to work with due to its sheer size (over 200 GB!). Text files of this size can’t be opened in R, Excel, or most other software. Fortunately, the R package auk has been specifically designed to extract subsets of data from the EBD for analysis using the Unix command line text processing utility AWK. The goal when using auk should always be to subset the full EBD down to a manageable size, small enough that it can be imported into R for further processing or analysis. In our case, that will mean extracting Wood Thrush records from BCR 27 in June. Filtering the EBD using auk requires three steps. First, reference the EBD and SED using the function auk_ebd(). If you’ve followed the instruction in the Introduction for downloading eBird data, you’ll have these files on your computer and will have pointed auk to the directory they’re stored in using auk_set_ebd_path(). If you intend to zero-fill the data, you’ll need to pass both the EBD and SED to auk_ebd(), this ensures that the same set of filters is applied to both files so they contain the same population of checklists. It is critical that the EBD and SED are both from the same release of the eBird data. If you don’t have enough hard drive space to store the full eBird data set, which is more than 200GB, or don’t have a fast enough internet connection to download it, consult the section on using the Custom Download form for an alternative approach to filtering eBird data. Before running any of the following code, create an RStudio project for following along with this book. The project will be a self contained space for all the code, input data, and outputs that comprise the lessons in this book. In addition, using a project will ensure your working directory is set to the project directory. library(auk) library(lubridate) library(sf) library(gridExtra) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select # setup data directory dir.create(&quot;data&quot;, showWarnings = FALSE) ebd &lt;- auk_ebd(&quot;ebd_relSep-2019.txt&quot;, file_sampling = &quot;ebd_sampling_relSep-2019.txt&quot;) Next, define the filters that you want to apply to the EBD. Each field that you can filter on has an associated function. For example, we’ll filter to Wood Thrush observations with auk_species(), from BCR 27 with auk_bcr(), in June of any year with auk_date(), restrict observations to those from either Stationary or Traveling protocols with auk_protocol(), and only keep complete checklists with auk_complete() since we intend to zero-fill the data. For a full list of possible filters, consult the package documentation. ebd_filters &lt;- ebd %&gt;% auk_species(&quot;Wood Thrush&quot;) %&gt;% # southeastern coastal plain bcr auk_bcr(bcr = 27) %&gt;% # june, use * to get data from any year auk_date(date = c(&quot;*-06-01&quot;, &quot;*-06-30&quot;)) %&gt;% # restrict to the standard traveling and stationary count protocols auk_protocol(protocol = c(&quot;Stationary&quot;, &quot;Traveling&quot;)) %&gt;% auk_complete() ebd_filters #&gt; Input #&gt; EBD: /Users/mes335/data/ebird/ebd_relSep-2019.txt #&gt; Sampling events: /Users/mes335/data/ebird/ebd_sampling_relSep-2019.txt #&gt; #&gt; Output #&gt; Filters not executed #&gt; #&gt; Filters #&gt; Species: Hylocichla mustelina #&gt; Countries: all #&gt; States: all #&gt; BCRs: 27 #&gt; Bounding box: full extent #&gt; Date: *-06-01 - *-06-30 #&gt; Start time: all #&gt; Last edited date: all #&gt; Protocol: Stationary, Traveling #&gt; Project code: all #&gt; Duration: all #&gt; Distance travelled: all #&gt; Records with breeding codes only: no #&gt; Complete checklists only: yes Note that printing the object ebd_filters shows what filters have been set. At this point, we’ve only defined the filters, not applied them to the EBD. The last step is to use auk_filter() to compile the filters into an AWK script and run it to produce two output files: one for the EBD and one for the SED. This step typically takes several hours to run since the files are so large. As a result, it’s wise to wrap this in an if statement, so it’s only run once. As noted in the Introduction, Windows users will need to install Cygwin for this next step to work. # output files data_dir &lt;- &quot;data&quot; if (!dir.exists(data_dir)) { dir.create(data_dir) } f_ebd &lt;- file.path(data_dir, &quot;ebd_woothr_june_bcr27.txt&quot;) f_sampling &lt;- file.path(data_dir, &quot;ebd_checklists_june_bcr27.txt&quot;) # only run if the files don&#39;t already exist if (!file.exists(f_ebd)) { auk_filter(ebd_filters, file = f_ebd, file_sampling = f_sampling) } These files are now a few megabytes rather than hundreds of gigabytes, which means they can easily be read into R! Don’t feel like waiting for auk_filter() to run? Download the data package mentioned in the introduction to get a copy of the EBD subset for Wood Thrush in June in BCR27 and proceed to the next section; just make sure you load the packages in the first code chunk before proceeding. 2.3 Importing and zero-filling The previous step left us with two tab separated text files, one for the EBD and one for the SED. Next, we’ll use auk_zerofill() to read these two files into R and combine them together to produce zero-filled, detection/non-detection data (also called presence/absence data). To just read the EBD or SED, but not combine them, use read_ebd() or read_sampling(), respectively. ebd_zf &lt;- auk_zerofill(f_ebd, f_sampling, collapse = TRUE) When any of the read functions from auk are used, two important processing steps occur by default behind the scenes. First, eBird observations can be made at levels below species (e.g. subspecies) or above species (e.g. a bird that was only identified as Duck sp.); however, for most uses we’ll want observations at the species level. auk_rollup() is applied by default when auk_zerofill() is used, and it drops all observations not identifiable to a species and rolls up all observations reported below species to the species level. eBird also allows for group checklists, those shared by multiple users. These checklists lead to duplication or near duplication of records within the dataset and the function auk_unique(), applied by default by auk_zerofill(), addresses this by only keeping one independent copy of each checklist. Finally, by default auk_zerofill() returns a compact representation of the data, consisting of a list of two data frames, one with checklist data and the other with observation data; the use of collapse = TRUE combines these into a single data frame, which will be easier to work with. Before continuing, we’ll transform some of the variables to a more useful form for modelling. We convert time to a decimal value between 0 and 24, and we force the distance travelled to 0 for stationary checklists. Notably, eBirders have the option of entering an “X” rather than a count for a species, to indicate that the species was present, but they didn’t keep track of how many individuals were observed. During the modeling stage, we’ll want the observation_count variable stored as an integer and we’ll convert “X” to NA to allow for this. # function to convert time observation to hours since midnight time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } # clean up variables ebd_zf &lt;- ebd_zf %&gt;% mutate( # convert X to NA observation_count = if_else(observation_count == &quot;X&quot;, NA_character_, observation_count), observation_count = as.integer(observation_count), # effort_distance_km to 0 for non-travelling counts effort_distance_km = if_else(protocol_type != &quot;Traveling&quot;, 0, effort_distance_km), # convert time to decimal hours since midnight time_observations_started = time_to_decimal(time_observations_started), # split date into year and day of year year = year(observation_date), day_of_year = yday(observation_date) ) 2.4 Accounting for variation in detectability As discussed in the Introduction, variation in effort between checklists makes inference challenging, because it is associated with variation in detectability. When working with semi-structured datasets like eBird, one approach to dealing with this variation is to impose some more consistent structure on the data by filtering observations on the effort variables. This reduces the variation in detectability between checklists. Based on our experience working with these data, we suggest restricting checklists to less than 5 hours long and 5 km in length, and with 10 or fewer observers. Furthermore, we’ll only consider data from the past 10 years (2010-2019). # additional filtering ebd_zf_filtered &lt;- ebd_zf %&gt;% filter( # effort filters duration_minutes &lt;= 5 * 60, effort_distance_km &lt;= 5, # last 10 years of data year &gt;= 2010, # 10 or fewer observers number_observers &lt;= 10) Finally, there are a large number of variables in the EBD that are redundant (e.g. both state names and codes are present) or unnecessary for most modeling exercises (e.g. checklist comments and Important Bird Area codes). These can be removed at this point, keeping only the variables we want for modelling. Then we’ll save the resulting zero-filled observations for use in later chapters. ebird &lt;- ebd_zf_filtered %&gt;% select(checklist_id, observer_id, sampling_event_identifier, scientific_name, observation_count, species_observed, state_code, locality_id, latitude, longitude, protocol_type, all_species_reported, observation_date, year, day_of_year, time_observations_started, duration_minutes, effort_distance_km, number_observers) write_csv(ebird, &quot;data/ebd_woothr_june_bcr27_zf.csv&quot;, na = &quot;&quot;) If you’d like to ensure you’re using exactly the same data as was used to generate this book, download the data package mentioned in the setup instructions and place the contents in the data/ subdirectory of your project directory. 2.5 Exploratory analysis and visualization Before proceeding to fitting species distribution models with these data, it’s worth exploring the dataset to see what we’re working with. Let’s start by making a simple map of the observations. This map uses GIS data available for download in the data package. Place the contents of the zip file in the data/ subdirectory of your project directory. # load and project gis data map_proj &lt;- st_crs(102003) ne_land &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_land&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() bcr &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;bcr&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_country_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_country_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_state_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_state_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() # prepare ebird data for mapping ebird_sf &lt;- ebird %&gt;% # convert to spatial points st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = map_proj) %&gt;% select(species_observed) # map par(mar = c(0.25, 0.25, 0.25, 0.25)) # set up plot area plot(st_geometry(ebird_sf), col = NA) # contextual gis data plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) plot(bcr, col = &quot;#cccccc&quot;, border = NA, add = TRUE) plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) plot(ne_country_lines, col = &quot;#ffffff&quot;, lwd = 1.5, add = TRUE) # ebird observations # not observed plot(st_geometry(ebird_sf), pch = 19, cex = 0.1, col = alpha(&quot;#555555&quot;, 0.25), add = TRUE) # observed plot(filter(ebird_sf, species_observed) %&gt;% st_geometry(), pch = 19, cex = 0.3, col = alpha(&quot;#4daf4a&quot;, 1), add = TRUE) # legend legend(&quot;bottomright&quot;, bty = &quot;n&quot;, col = c(&quot;#555555&quot;, &quot;#4daf4a&quot;), legend = c(&quot;eBird checklists&quot;, &quot;Wood Thrush sightings&quot;), pch = 19) box() par(new = TRUE, mar = c(0, 0, 3, 0)) title(&quot;Wood Thrush eBird Observations\\nJune 2010-2019, BCR 27&quot;) In this map, the spatial bias in eBird data becomes immediately obvious, for example, notice the large number of checklists along the heavily populated Atlantic coast, especially around large cities like Jacksonville, Florida and popular birding areas like the Outer Banks of North Carolina. Exploring the effort variables is also a valuable exercise. For each effort variable, we’ll produce both a histogram and a plot of frequency of detection as a function of that effort variable. The histogram will tell us something about birder behavior. For example, what time of day are most people going birding, and for how long? We may also want to note values of the effort variable that have very few observations; predictions made in these regions may be unreliable due to a lack of data. The detection frequency plots tell us how the probability of detecting a species changes with effort. 2.5.1 Time of day The chance of an observer detecting a bird when present can be highly dependent on time of day. For example, many species exhibit a peak in detection early in the morning during dawn chorus and a secondary peak early in the evening. With this in mind, the first predictor of detection that we’ll explore is the time of day at which a checklist was started. We’ll summarize the data in 1 hour intervals, then plot them. Since estimates of detection frequency are unreliable when only a small number of checklists are available, we’ll only plot hours for which at least 100 checklists are present. # summarize data by hourly bins breaks &lt;- 0:24 labels &lt;- breaks[-length(breaks)] + diff(breaks) / 2 ebird_tod &lt;- ebird %&gt;% mutate(tod_bins = cut(time_observations_started, breaks = breaks, labels = labels, include.lowest = TRUE), tod_bins = as.numeric(as.character(tod_bins))) %&gt;% group_by(tod_bins) %&gt;% summarise(n_checklists = n(), n_detected = sum(species_observed), det_freq = mean(species_observed)) # histogram g_tod_hist &lt;- ggplot(ebird_tod) + aes(x = tod_bins, y = n_checklists) + geom_col(width = mean(diff(breaks)), color = &quot;grey30&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(0, 24, by = 3), limits = c(0, 24)) + scale_y_continuous(labels = scales::comma) + labs(x = &quot;Hours since midnight&quot;, y = &quot;# checklists&quot;, title = &quot;Distribution of observation start times&quot;) # frequency of detection g_tod_freq &lt;- ggplot(ebird_tod %&gt;% filter(n_checklists &gt; 100)) + aes(x = tod_bins, y = det_freq) + geom_line() + geom_point() + scale_x_continuous(breaks = seq(0, 24, by = 3), limits = c(0, 24)) + scale_y_continuous(labels = scales::percent) + labs(x = &quot;Hours since midnight&quot;, y = &quot;% checklists with detections&quot;, title = &quot;Detection frequency&quot;) # combine grid.arrange(g_tod_hist, g_tod_freq) As expected, Wood Thrush detectability is highest early in the morning and quickly falls off as the day progresses. In later chapters, we’ll make predictions at the peak time of day for detecatibility to limit the effect of this variation. The majority of checklist submissions also occurs in the morning; however, there are reasonable numbers of checklists between 5am and 9pm. It’s in this region that our model estimates will be most reliable. 2.5.2 Checklist duration When we initially extracted the eBird data in Section 2.2, we restricted observations to those from checklists 5 hours in duration or shorter to reduce variability. Let’s see what sort of variation remains in checklist duration. # summarize data by 30 minute bins breaks &lt;- seq(0, 5, by = 0.5) labels &lt;- breaks[-length(breaks)] + diff(breaks) / 2 ebird_dur &lt;- ebird %&gt;% mutate(dur_bins = cut(duration_minutes / 60, breaks = breaks, labels = labels, include.lowest = TRUE), dur_bins = as.numeric(as.character(dur_bins))) %&gt;% group_by(dur_bins) %&gt;% summarise(n_checklists = n(), n_detected = sum(species_observed), det_freq = mean(species_observed)) # histogram g_dur_hist &lt;- ggplot(ebird_dur) + aes(x = dur_bins, y = n_checklists) + geom_col(width = mean(diff(breaks)), color = &quot;grey30&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = 0:5) + scale_y_continuous(labels = scales::comma) + labs(x = &quot;Checklist duration (hours)&quot;, y = &quot;# checklists&quot;, title = &quot;Distribution of checklist durations&quot;) # frequency of detection g_dur_freq &lt;- ggplot(ebird_dur %&gt;% filter(n_checklists &gt; 100)) + aes(x = dur_bins, y = det_freq) + geom_line() + geom_point() + scale_x_continuous(breaks = 0:5) + scale_y_continuous(labels = scales::percent) + labs(x = &quot;Checklist duration (hours)&quot;, y = &quot;% checklists with detections&quot;, title = &quot;Detection frequency&quot;) # combine grid.arrange(g_dur_hist, g_dur_freq) The majority of checklists are half an hour or shorter and there is a rapid decline in the frequency of checklists with increasing duration. In addition, longer searches yield a higher chance of detecting a Wood Thrush. In many cases, there is a saturation effect, with searches beyond a given length producing little additional benefit; however, here there appears to be a drop off in detection for checklists longer than 4 hours. 2.5.3 Distance traveled As with checklist duration, we expect a priori that the greater the distance someone travels, the greater the probability of encountering at least one Wood Thrush. Let’s see if this expectation is met. Note that we have already truncated the data to checklists less than 5 km in length. # summarize data by 500m bins breaks &lt;- seq(0, 5, by = 0.5) labels &lt;- breaks[-length(breaks)] + diff(breaks) / 2 ebird_dist &lt;- ebird %&gt;% mutate(dist_bins = cut(effort_distance_km, breaks = breaks, labels = labels, include.lowest = TRUE), dist_bins = as.numeric(as.character(dist_bins))) %&gt;% group_by(dist_bins) %&gt;% summarise(n_checklists = n(), n_detected = sum(species_observed), det_freq = mean(species_observed)) # histogram g_dist_hist &lt;- ggplot(ebird_dist) + aes(x = dist_bins, y = n_checklists) + geom_col(width = mean(diff(breaks)), color = &quot;grey30&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = 0:5) + scale_y_continuous(labels = scales::comma) + labs(x = &quot;Distance travelled (km)&quot;, y = &quot;# checklists&quot;, title = &quot;Distribution of distance travelled&quot;) # frequency of detection g_dist_freq &lt;- ggplot(ebird_dist %&gt;% filter(n_checklists &gt; 100)) + aes(x = dist_bins, y = det_freq) + geom_line() + geom_point() + scale_x_continuous(breaks = 0:5) + scale_y_continuous(labels = scales::percent) + labs(x = &quot;Distance travelled (km)&quot;, y = &quot;% checklists with detections&quot;, title = &quot;Detection frequency&quot;) # combine grid.arrange(g_dist_hist, g_dist_freq) As with duration, the majority of observations are from short checklists (less than half a kilometer). One fortunate consequence of this is that most checklists will be contained within a small area within which habitat is not likely to show high variability. In chapter @ref{covariates}, we will summarize land cover data within circles 2.5 km in diameter, centered on each checklist, and it appears that the vast majority of checklists will stay contained within this area. 2.5.4 Number of observers {#ebird-explore-observers,} Finally, let’s consider the number of observers whose observation are being reported in each checklist. We expect that at least up to some number of observers, reporting rates will increase; however, in working with these data we have found cases of declining detection rates for very large groups. With this in mind we have already restricted checklists to those with 10 or fewer observers, thus removing the very largest groups (prior to filtering, some checklists had as many as 230 observers!). # summarize data breaks &lt;- 0:10 labels &lt;- 1:10 ebird_obs &lt;- ebird %&gt;% mutate(obs_bins = cut(number_observers, breaks = breaks, label = labels, include.lowest = TRUE), obs_bins = as.numeric(as.character(obs_bins))) %&gt;% group_by(obs_bins) %&gt;% summarise(n_checklists = n(), n_detected = sum(species_observed), det_freq = mean(species_observed)) # histogram g_obs_hist &lt;- ggplot(ebird_obs) + aes(x = obs_bins, y = n_checklists) + geom_col(width = mean(diff(breaks)), color = &quot;grey30&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = 1:10) + scale_y_continuous(labels = scales::comma) + labs(x = &quot;# observers&quot;, y = &quot;# checklists&quot;, title = &quot;Distribution of the number of observers&quot;) # frequency of detection g_obs_freq &lt;- ggplot(ebird_obs %&gt;% filter(n_checklists &gt; 100)) + aes(x = obs_bins, y = det_freq) + geom_line() + geom_point() + scale_x_continuous(breaks = 1:10) + scale_y_continuous(labels = scales::percent) + labs(x = &quot;# observers&quot;, y = &quot;% checklists with detections&quot;, title = &quot;Detection frequency&quot;) # combine grid.arrange(g_obs_hist, g_obs_freq) There is no discernable pattern amongst the noise here, likely because there are so few checklists with more than 3 observers. 2.6 EBD file size issues The full EBD is massive (over 200 GB!) and takes a long time to process with auk (typically several hours). Furthermore, it’s onerous to download and uncompress the file, and many users won’t have space on their hard drive to store the file. Fortunately, there are a variety of ways of addressing these file size issues. In this section, we’ll demonstrate how to use the Custom Download form to download only a subset of the eBird data. In addition, we’ll cover some methods for reducing the size of both the EBD and extracts from the EBD. Specifically, we’ll cover stricter filtering, removing columns, and splitting by species. 2.6.1 Custom downloads On the EBD download page, there’s a Custom Download form that allows you to request a subset of the EBD for a given species, within a region, and/or for a range of dates. After submitting a request, it will be processed on the eBird servers and an email will be sent to you with instructions for downloading the EBD extract. In many cases, using the custom download form can circumvent the need for downloading the full EBD; however, there are two important caveats. First, this approach doesn’t give the full range of filters that are available in auk, it only allows you to get data for a single species, within a single region, for a range of dates. Second, a custom download request will only provide observation data from the EBD, the associated Sampling Event Data (SED) required for zero-filling is not provided. Fortunately, both these issues can be addressed using the following method. Submit a custom download request that narrows down the data you’ll receive as much as possible. For the example in this chapter, we’ll request Wood Thrush data from the United States (since BCR 27 is entirely within the US). Note that the custom download form has a date filter, but it allows us to select data from a range of dates, hence we can’t specify June of any year, so we’ll leave this blank. Fill out the form as in the above image. If you intend to later zero-fill these data, it’s critical that you don’t apply stricter filters in the custom download form than you eventually intend to use during auk filtering. You will receive an email (typically within 15 minutes) with instructions for downloading the requested data. Download and uncompress the data, then place the resulting data file in the data/ subdirectory of your RStudio project. The data will be in a file named similarly to the EBD, for example, ebd_US_woothr_relSep-2019.txt. Note the date in the filename (Sep-2019 in this case), which specifies the version of the EBD we’re working with. Also, note the size of this file is about 300 MB, nearly 1000 times smaller than the full EBD! Ensure that you have a copy of the SED that is the same version as the file you just downloaded. In particular, the two files should have the same date in their filenames (Sep-2019 in our example). If you don’t already have the correct SED version, download a fresh copy and place it in the EBD directory we specified in the introduction (you can see the location of this directory with auk::auk_get_ebd_path()). Now you can follow along with this chapter, starting at the beginning, with one simple change. In the first chunk of R code, when the ebd object is defined, change the EBD filename to the file from the custom download request. ebd &lt;- auk_ebd(&quot;data/ebd_US_woothr_relSep-2019.txt&quot;, file_sampling = &quot;ebd_sampling_relSep-2019.txt&quot;) Using the above steps, we can produce the exact same results without have to download and filter the the full EBD! 2.6.2 Stricter filtering If, after filtering, you’re EBD extract is still too large to read into R, the most obvious way to reduce the size is to use stricter filters: focus on a smaller region, shorter time period, or fewer species. To avoid having to waste several hours trying to filter the entire EBD all over again, it’s worthwhile noting that you can always re-filter an EBD extract directly. So, if you realize you were too coarse in your initial filtering, apply the stricter filter to the EBD extract rather than the full EBD so save time. 2.6.3 Removing columns The EBD contains a lot of columns (46 to be precise), many of which are redundant or not useful in most scenarios. For example, country, state, and county each have two columns, one for the name and one for the code. Other columns, such as the Important Bird Area (IBA) that a checklist belong to and the checklist comments, are rarely useful. By removing these columns we can drastically reduce the size of the EBD. The available columns in the EBD are listed and defined in the PDF metadata that comes with the EBD (eBird_Basic_Dataset_Metadata_v1.12.pdf). Alternatively, there’s a useful trick to get a list of column names from an auk_ebd object. ebd$col_idx$name #&gt; [1] &quot;global unique identifier&quot; &quot;last edited date&quot; #&gt; [3] &quot;taxonomic order&quot; &quot;category&quot; #&gt; [5] &quot;common name&quot; &quot;scientific name&quot; #&gt; [7] &quot;subspecies common name&quot; &quot;subspecies scientific name&quot; #&gt; [9] &quot;observation count&quot; &quot;breeding bird atlas code&quot; #&gt; [11] &quot;breeding bird atlas category&quot; &quot;age sex&quot; #&gt; [13] &quot;country&quot; &quot;country code&quot; #&gt; [15] &quot;state&quot; &quot;state code&quot; #&gt; [17] &quot;county&quot; &quot;county code&quot; #&gt; [19] &quot;iba code&quot; &quot;bcr code&quot; #&gt; [21] &quot;usfws code&quot; &quot;atlas block&quot; #&gt; [23] &quot;locality&quot; &quot;locality id&quot; #&gt; [25] &quot;locality type&quot; &quot;latitude&quot; #&gt; [27] &quot;longitude&quot; &quot;observation date&quot; #&gt; [29] &quot;time observations started&quot; &quot;observer id&quot; #&gt; [31] &quot;sampling event identifier&quot; &quot;protocol type&quot; #&gt; [33] &quot;protocol code&quot; &quot;project code&quot; #&gt; [35] &quot;duration minutes&quot; &quot;effort distance km&quot; #&gt; [37] &quot;effort area ha&quot; &quot;number observers&quot; #&gt; [39] &quot;all species reported&quot; &quot;group identifier&quot; #&gt; [41] &quot;has media&quot; &quot;approved&quot; #&gt; [43] &quot;reviewed&quot; &quot;reason&quot; #&gt; [45] &quot;trip comments&quot; &quot;species comments&quot; The function auk_select() will process the EBD to only keep the selected columns. For example, we can apply this to our Wood Thrush EBD extract to keep a minimal set of columns cols &lt;- c(&quot;latitude&quot;, &quot;longitude&quot;, &quot;group identifier&quot;, &quot;sampling event identifier&quot;, &quot;scientific name&quot;, &quot;observation count&quot;, &quot;observer_id&quot;) f_select &lt;- &quot;data/ebd_smaller.txt&quot; selected &lt;- auk_ebd(f_ebd) %&gt;% auk_select(select = cols, file = f_select) %&gt;% read_ebd() glimpse(selected) #&gt; Observations: 3,955 #&gt; Variables: 8 #&gt; $ checklist_id &lt;chr&gt; &quot;S8861448&quot;, &quot;S11177008&quot;, &quot;S20926623&quot;, &quot;S14628278&quot;, &quot;S20… #&gt; $ scientific_name &lt;chr&gt; &quot;Hylocichla mustelina&quot;, &quot;Hylocichla mustelina&quot;, &quot;Hyloci… #&gt; $ observation_count &lt;chr&gt; &quot;45&quot;, &quot;4&quot;, &quot;1&quot;, &quot;6&quot;, &quot;1&quot;, &quot;4&quot;, &quot;1&quot;, &quot;1&quot;, &quot;7&quot;, &quot;2&quot;, &quot;1&quot;,… #&gt; $ latitude &lt;dbl&gt; 35.5, 35.1, 32.8, 35.3, 32.9, 35.3, 34.6, 32.7, 34.9, 3… #&gt; $ longitude &lt;dbl&gt; -89.2, -90.1, -87.2, -90.1, -87.2, -90.0, -88.2, -87.2,… #&gt; $ observer_id &lt;chr&gt; &quot;obsr245742&quot;, &quot;obsr245742&quot;, &quot;obsr35725&quot;, &quot;obsr245742&quot;, … #&gt; $ sampling_event_identifier &lt;chr&gt; &quot;S8861448&quot;, &quot;S11177008&quot;, &quot;S20926623&quot;, &quot;S14628278&quot;, &quot;S20… #&gt; $ group_identifier &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… # file size difference file.size(f_ebd) / file.size(f_select) #&gt; [1] 5.72 So, by removing columns, we’ve reduced the size of the EBD extract by a factor of almost 6. auk_select() is typically applied to a subset of the EBD to reduce the size before reading it into R. However, this function can also be applied to entire EBD to remove any columns that you’re sure you’ll never need. If you’re running out of hard drive space, this approach can drastically reduce the size of the EBD and free up space. Selecting columns can also be done simultaneously with filtering by using the keep and drop arguments to auk_fiter(). Consult the function documentation for details. 2.6.4 Splitting by species If you’re working with a large number of species, the size of extracts from the global EBD can quickly increase. An easy way to address this before importing data into R is to split the EBD extract up into separate files, one for each species. You can then read in species one at a time and process them. Note that the Sampling Event Data doesn’t need to be split and the same file can be used for each species EBD file. The function auk_split() facilitates this process. You provide an EBD extract and a vector of species names and it will split the EBD up into species-specific files. This approach is much faster because we only need to filter the EBD once instead of once for each species. This could be the difference between a few hours of processing time and a few days. Consult the auk_split() function documentation for details. References "],
["covariates.html", "Chapter 3 Habitat Covariates 3.1 Introduction 3.2 Downloading MODIS data 3.3 Landscape metrics 3.4 Prediction surface 3.5 Elevation", " Chapter 3 Habitat Covariates 3.1 Introduction Species distribution models work by finding associations between species occurrence or abundance and environmental variables. Using these relationships, it’s possible to predict the distribution in areas that aren’t sampled, provided we know the value of the environmental variables in these areas. Therefore, to proceed with the modeling in the next several chapters, we’ll need to prepare a suite of environmental variables to be used as covariates in our models. The particular set of covariates that’s most suitable for a given study will depend on the focal species, region, and time period, as well as the availability of data. When species distributions are well defined by the environmental covariates, extrapolations to unsurveyed areas will be more accurate. So, itss worth considering which environmental covariates are important for your species. Scientists can use many variables to characterise a species distribution - for example, climate, weather, and soil type. Here we use only landcover and elevation as example environmental covariates. Fortunately, there are an abundance of freely available, satellite-based land cover products derived from satellites such as Landsat, SPOT, and MODIS that are suitable for distribution modeling. This land cover data will act as a proxy for habitat and throughout this book we’ll often use habitat and land cover interchangeably. In addition, we’ll include elevation as an additional covariate, which can be important for many species. For the examples in this book, we’ll use land cover covariates derived from the MODIS MCD12Q1 v006 land cover product (Friedl and Sulla-Menashe 2015). This product has global coverage at 500 m spatial resolution and annual temporal resolution from 2001-2018. These data are available for several different classification schemes. We’ll use the University of Maryland (UMD) land cover classification, which provides a globally accurate classification of land cover in our experience. This system classifies pixels into one of 16 different land cover classes: Class Name Description 0 Water bodies At least 60% of area is covered by permanent water bodies. 1 Evergreen Needleleaf Forests Dominated by evergreen conifer trees (canopy &gt;2m). Tree cover &gt;60%. 2 Evergreen Broadleaf Forests Dominated by evergreen broadleaf and palmate trees (canopy &gt;2m). Tree cover &gt;60%. 3 Deciduous Needleleaf Forests Dominated by deciduous needleleaf (e.g. larch) trees (canopy &gt;2m). Tree cover &gt;60%. 4 Deciduous Broadleaf Forests Dominated by deciduous broadleaf trees (canopy &gt;2m). Tree cover &gt;60%. 5 Mixed Forests Dominated by neither deciduous nor evergreen (40-60% of each) tree type (canopy &gt;2m). Tree cover &gt;60%. 6 Closed Shrublands Dominated by woody perennials (1-2m height) &gt;60% cover. 7 Open Shrublands Dominated by woody perennials (1-2m height) 10-60% cover. 8 Woody Savannas Tree cover 30-60% (canopy &gt;2m). 9 Savannas Tree cover 10-30% (canopy &gt;2m). 10 Grasslands Dominated by herbaceous annuals (&lt;2m). 11 Permanent Wetlands Permanently inundated lands with 30-60% water cover and &gt;10% vegetated cover. 12 Croplands At least 60% of area is cultivated cropland. 13 Urban and Built-up Lands At least 30% impervious surface area including building materials, asphalt, and vehicles. 14 Cropland/Natural Vegetation Mosaics Mosaics of small-scale cultivation 40-60% with natural tree, shrub, or herbaceous vegetation. 15 Non-Vegetated Lands At least 60% of area is non-vegetated barren (sand, rock, soil) or permanent snow and ice with less than 10% vegetation. 255 Unclassified Has not received a map label because of missing inputs. For a wide range of studies, this MODIS land cover dataset will be suitable for generating habitat covariates; however, there may be particular cases where the study species, habitat, or ecological question requires different, or more specialized, data. For example, shorebird distribution modeling would benefit from data on the extent of tidal flats, seabirds distributions are often influenced by ocean depth, and in many regions elevation plays a critical role in shaping species distributions. Regardless of which habitat data you decide to use for your project, this chapter should provide a template for how to prepare these data as covariates for modeling species distributions. The following section will cover how to access and download MODIS land cover data. Next, we’ll demonstrate how to summarize these data within a neighborhood around each checklist location. Then, we’ll calculate a set of covariates over a regular grid, which we’ll use to make predictions of species distributions throughout our study area. Finally, as an example of including covariate data from multiple sources, we’ll demonstrate how to incorporate elevation data as an additional covariate. If you want to skip this section and jump straight to the modeling, you can download the data package, which includes all the prepared MODIS data that we’ll use in the remainder of this book. 3.2 Downloading MODIS data As with most satellite data, MODIS data are provided as 1200 km by 1200 km tiles for ease of download. Each tile is a raster GIS dataset consisting of a regular grid of 500 m resolution cells. The surface of the Earth is divided up into a grid of these tiles, each given an ID, for example, h10v12 is the tile from the 10th column and 12th row of the grid. Compiling MODIS data for a given region requires figuring out which set of tiles covers the region, downloading those tiles, combining the tiles together into a single raster dataset, and converting from the native MODIS HDF format, which R can’t read, to a standard GeoTIFF format. This needs to be done for each year for which we want habitat data, and can be a time consuming and error prone process. Fortunately, the R package MODIS automates most of these steps. Unfortunately, this package can be challenging and confusing to get working. With this in mind, this section will provide detailed instruction for setting up and using the MODIS package. Let’s start by figuring out the tile IDs for the tiles that BCR 27 spans. Recall that we prepared a BCR boundary in Section 1.3.6 of the Introduction; if you haven’t already done so, download the data package now to get that boundary. Given a set of spatial features, the MODIS package can quickly tell us which MODIS tiles we need. library(sf) library(raster) library(MODIS) library(velox) library(viridis) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select projection &lt;- raster::projection # bcr 27 boundary bcr &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;bcr&quot;) %&gt;% filter(bcr_code == 27) %&gt;% # project to the native modis projection st_transform(crs = paste(&quot;+proj=sinu +lon_0=0 +x_0=0 +y_0=0&quot;, &quot;+a=6371007.181 +b=6371007.181 +units=m +no_defs&quot;)) # load ebird data ebird &lt;- read_csv(&quot;data/ebd_woothr_june_bcr27_zf.csv&quot;) # get list of tiles required to cover this bcr tiles &lt;- getTile(bcr) tiles@tile #&gt; [1] &quot;h10v06&quot; &quot;h10v05&quot; &quot;h11v05&quot; So, we’ll need to download these three tiles for each of the 10 years from 2010-2019. 3.2.1 MODIS setup Before we start using MODIS for the first time, a bit of setup is required. First, sign up for a NASA Earthdata account to get access to MODIS, and other NASA data. Then use MODIS::EarthdataLogin(usr = &quot;username&quot;, pwd = &quot;password&quot;), with the username and password you just created, to store your login credentials so the MODIS package can access them. Next, you’ll need to install GDAL, an open source library for working with geospatial data that’s needed for processing the MODIS tiles. The steps for installing GDAL are system dependent: Mac OS X: First, check if GDAL is installed with HDF4 support by running gdal-config --formats in Terminal. If you see hdf4 in the list, you don’t need to do anything else! If not, install the Homebrew package manager by following the instructions on the website. Then, run the following commands in Terminal to install GDAL: brew tap osgeo/osgeo4mac brew install hdf4 brew link --overwrite hdf4 brew install osgeo-gdal brew link --force osgeo-gdal Windows: install GDAL using OSGeo4W, a suite of open source geospatial tools. In R, run MODIS:::checkTools(&quot;GDAL&quot;), which will search your system for GDAL and suggest a command such as MODIS::MODISoptions(gdalPath = &quot;c:/OSGeo4W64/bin&quot;) that will make GDAL available to the MODIS package. Run this command and, when it asks, agree to making the settings permanent. Linux: run sudo apt-get install gdal-bin in the terminal. Finally, run MODIS:::checkTools(&quot;GDAL&quot;) to check that GDAL is installed and that the MODIS package can find it. If GDAL can’t be found, you’ll need to manually locate it and use MODIS::MODISoptions(gdalPath = &quot;path/to/gdal/&quot;) to tell the MODIS package where it is. 3.2.2 Download using R Once all the setup steps have been completed, we can start downloading some data! The MODIS function runGdal() downloads and processes MODIS tiles into a single GeoTIFF for each year. Note that at the time of writing, land cover data from 2019 haven’t been prepared yet, so we’ll use 2018 data for both 2018 and 2019. The key arguments to runGdal() are: product: is the specific MODIS product to download. For a full list of available datasets use MODIS::getProduct(). collection: each MODIS product may have multiple collections, corresponding roughly to versions. Use getCollection() to find the available collection for a given product. SDSstring: a string specifying which bands to extract, with zeros for bands to drop and 1 for bands to keep. Most MODIS products have multiple bands stored in a single raster file, for example, reflectances in different wavelength ranges or, in our case, land cover using different land cover classification systems. The documentation for the MCD12Q1 dataset shows that there are 13 bands in the downloaded files, and we’re interested in band 2, which contains the UMD landcover classification. extent: any of several different spatial objects specifying the region that we want data for. In our case, we’ll use the BCR polygon; however, for a list of available options consult the help for getTile(). Note that runGdal() will return raster data in the same projection as the input extent, which is why we projected the BCR boundary to the MODIS sinusoidal projection. begin and end: the start and end dates of the time period from which to extract data. Although the land cover data are only available annually, we need to specify full dates because some other products are available on a more granular basis. outDirPath: directory to store processed MODIS data. job: a name for this task, which will become the sub-directory of outDirPath within which the processed data are stored. # earliest year of ebird data begin_year &lt;- format(min(ebird$observation_date), &quot;%Y.01.01&quot;) # end date for ebird data end_year &lt;- format(max(ebird$observation_date), &quot;%Y.12.31&quot;) # download tiles and combine into a single raster for each year tifs &lt;- runGdal(product = &quot;MCD12Q1&quot;, collection = &quot;006&quot;, SDSstring = &quot;01&quot;, extent = bcr %&gt;% st_buffer(dist = 10000), begin = begin_year, end = end_year, outDirPath = &quot;data&quot;, job = &quot;modis&quot;, MODISserverOrder = &quot;LPDAAC&quot;) %&gt;% pluck(&quot;MCD12Q1.006&quot;) %&gt;% unlist() # rename tifs to have more descriptive names new_names &lt;- format(as.Date(names(tifs)), &quot;%Y&quot;) %&gt;% sprintf(&quot;modis_mcd12q1_umd_%s.tif&quot;, .) %&gt;% file.path(dirname(tifs), .) file.rename(tifs, new_names) If everything ran smoothly, we now have annual GeoTIFFs of MODIS land cover data that we can load into R. You may see error messages stating Cannot find proj.db, or something similar, these can be safely ignored provided the modis have been created in data/modis/ directory. # load the landcover data landcover &lt;- list.files(&quot;data/modis&quot;, &quot;^modis_mcd12q1_umd&quot;, full.names = TRUE) %&gt;% stack() # label layers with year landcover &lt;- names(landcover) %&gt;% str_extract(&quot;(?&lt;=modis_mcd12q1_umd_)[0-9]{4}&quot;) %&gt;% paste0(&quot;y&quot;, .) %&gt;% setNames(landcover, .) landcover #&gt; class : RasterStack #&gt; dimensions : 1969, 4301, 8468669, 9 (nrow, ncol, ncell, nlayers) #&gt; resolution : 463, 463 (x, y) #&gt; extent : -8745491, -6752783, 3242262, 4154525 (xmin, xmax, ymin, ymax) #&gt; crs : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs #&gt; names : y2010, y2011, y2012, y2013, y2014, y2015, y2016, y2017, y2018 #&gt; min values : 0, 0, 0, 0, 0, 0, 0, 0, 0 #&gt; max values : 255, 255, 255, 255, 255, 255, 255, 255, 255 These data have not been prepared yet for the last couple years, so we’ll need to fill in the missing years using the most recent year for which there is data. To facilitate that, let’s figure out which is the most recent year with data. max_lc_year &lt;- names(landcover) %&gt;% str_extract(&quot;[0-9]{4}&quot;) %&gt;% as.integer() %&gt;% max() So, we have landcover data up to 2018. 3.2.3 Troubleshooting If the call to runGDAL() didn’t work for you, don’t worry, you’re not alone! It’s challenging to get the MODIS package working and errors are common when you’re first trying to get it set up. The most common error is not having GDAL installed correctly, which will give an error like GDAL not installed or configured. Either you don’t have GDAL at all or you have it, but it doesn’t have support for HDF4 files (this is the native format for MODIS data). Try following the above instructions again. If it still doesn’t work, consult the instructions on the MODIStsp website for installing GDAL. Another error you may see is: Make sure either 'wget' or 'curl' is available in order to download data from LP DAAC or NSIDC.. This should only arise on versions of Windows before Windows 10. If you see this error, you’ll need to install curl, which is used by R to download the MODIS tiles. There is a StackOverflow question with excellent instructions for installing curl and getting it setup on your system. If these tips haven’t solved your particular problem, you’ll need to turn to Google to troubleshoot or find someone who has experience with these tools and ask them to help. Good luck! 3.3 Landscape metrics At this point we could use the MODIS land cover data directly, simply extracting the land cover class for each checklist location. However, we instead advocate summarizing the land cover data within a neighborhood around the checklist locations. As discussed in Section 1.1, checklist locations are not precise, so it’s more appropriate to use the habitat in the surrounding area, rather than only at the checklist location. More fundamentally, organisms interact with their environment not at a single point, but at the scale of a landscape, so it’s important to include habitat information characterizing a suitably-sized landscape around the observation location. There are a variety of landscape metrics that can be used to characterize the composition (what habitat is available) and configuration (how that habitat is arranged spatially) of landscapes. The simplest metric of landscape composition is the proportion of the landscape in each land cover class (PLAND in the parlance of FRAGSTATS). For a broad range of scenarios, PLAND is a reliable choice for calculating habitat covariates in distribution modeling. Based on our experience working with eBird data, an approximately 2.5 km by 2.5 km neighborhood (5 by 5 MODIS cells) centered on the checklist location is sufficient to account for the spatial precision in the data when the maximum distance of travelling counts has been limited to 5 km, while being a relevant ecological scale for many bird species. We’ll start by finding the full set of unique checklists locations for each year in the eBird data. Then we convert these locations to spatial sf features and project them to the sinusoidal equal area projection used by MODIS. We’ll buffer these points to create a neighborhood around each location with a diamter equal to 5 MODIS cells. Finally, we split the neighborhoods up by year so we can match to MODIS land cover data from the corresponding year. neighborhood_radius &lt;- 5 * ceiling(max(res(landcover))) / 2 ebird_buff &lt;- ebird %&gt;% distinct(year = format(observation_date, &quot;%Y&quot;), locality_id, latitude, longitude) %&gt;% # for 2018 use 2017 landcover data mutate(year_lc = if_else(as.integer(year) &gt; max_lc_year, as.character(max_lc_year), year), year_lc = paste0(&quot;y&quot;, year_lc)) %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% # transform to modis projection st_transform(crs = projection(landcover)) %&gt;% # buffer to create neighborhood around each point st_buffer(dist = neighborhood_radius) %&gt;% # nest by year nest(data = c(year, locality_id, geometry)) Now, we’ll loop over the years and for each square neighborhood extract all the raster values within that neighborhood. We use the velox package for this, since it’s often orders of magnitude faster than using raster::extract(). # function to extract landcover data for all checklists in a given year calculate_pland &lt;- function(yr, regions, lc) { # create a lookup table to get locality_id from row number locs &lt;- st_set_geometry(regions, NULL) %&gt;% mutate(id = row_number()) # extract using velox lc_vlx &lt;- velox(lc[[yr]]) lc_vlx$extract(regions, df = TRUE) %&gt;% # velox doesn&#39;t properly name columns, fix that set_names(c(&quot;id&quot;, &quot;landcover&quot;)) %&gt;% # join to lookup table to get locality_id inner_join(locs, ., by = &quot;id&quot;) %&gt;% select(-id) } # iterate over all years extracting landcover for all checklists in each lc_extract &lt;- ebird_buff %&gt;% mutate(pland = map2(year_lc, data, calculate_pland, lc = landcover)) %&gt;% select(pland) %&gt;% unnest(cols = pland) Now we have the set of land cover values within a neighborhood around each checklist location. We can summarize these data within each neighborhood to calculate PLAND: the proportion of the neighborhood within each land cover class. pland &lt;- lc_extract %&gt;% # count landcovers count(locality_id, year, landcover) %&gt;% # calculate proporiton group_by(locality_id, year) %&gt;% mutate(pland = n / sum(n)) %&gt;% ungroup() %&gt;% select(-n) %&gt;% # remove NAs after tallying so pland is relative to total number of cells filter(!is.na(landcover)) Finally, we’ll convert the numeric landcover codes to more descriptive names and transform the data to a wide format with each row a location and the PLAND values in columns. # convert names to be more descriptive lc_names &lt;- tibble(landcover = 0:15, lc_name = c(&quot;pland_00_water&quot;, &quot;pland_01_evergreen_needleleaf&quot;, &quot;pland_02_evergreen_broadleaf&quot;, &quot;pland_03_deciduous_needleleaf&quot;, &quot;pland_04_deciduous_broadleaf&quot;, &quot;pland_05_mixed_forest&quot;, &quot;pland_06_closed_shrubland&quot;, &quot;pland_07_open_shrubland&quot;, &quot;pland_08_woody_savanna&quot;, &quot;pland_09_savanna&quot;, &quot;pland_10_grassland&quot;, &quot;pland_11_wetland&quot;, &quot;pland_12_cropland&quot;, &quot;pland_13_urban&quot;, &quot;pland_14_mosiac&quot;, &quot;pland_15_barren&quot;)) pland &lt;- pland %&gt;% inner_join(lc_names, by = &quot;landcover&quot;) %&gt;% arrange(landcover) %&gt;% select(-landcover) # tranform to wide format, filling in implicit missing values with 0s%&gt;% pland &lt;- pland %&gt;% pivot_wider(names_from = lc_name, values_from = pland, values_fill = list(pland = 0)) # save write_csv(pland, &quot;data/modis_pland_location-year.csv&quot;) 3.4 Prediction surface After fitting species distribution models, the goal is typically to make predictions throughout the study area. To do this, we’ll need a regular grid of habitat covariates over which to make predictions. In this section, we’ll create such a prediction surface for BCR 27 using the MODIS land cover data from the most recent year for which they’re available. To start, we’ll need a template raster with cells equal in size to the neighborhoods we defined in the previous section: 5 by 5 MODIS land cover cells. We can use raster::aggregate() to achieve this. We’ll also use raster::rasterize() to assign the value 1 to all cells within BCR 27 and leave all cells outside BCR 27 empty. agg_factor &lt;- round(2 * neighborhood_radius / res(landcover)) r &lt;- raster(landcover) %&gt;% aggregate(agg_factor) r &lt;- bcr %&gt;% st_transform(crs = projection(r)) %&gt;% rasterize(r, field = 1) %&gt;% # remove any empty cells at edges trim() r &lt;- writeRaster(r, filename = &quot;data/prediction-surface.tif&quot;, overwrite = TRUE) Next, for each cell of this raster, we’ll calculate the PLAND metrics using the same approach as the previous section. Note that we will only be creating this prediction surface for the most current year of landcover data in our example. # get cell centers and create neighborhoods r_centers &lt;- rasterToPoints(r, spatial = TRUE) %&gt;% st_as_sf() %&gt;% transmute(id = row_number()) r_cells &lt;- st_buffer(r_centers, dist = neighborhood_radius) # extract landcover values within neighborhoods, only needed most recent year lc_vlx &lt;- velox(landcover[[paste0(&quot;y&quot;, max_lc_year)]]) lc_extract_pred &lt;- lc_vlx$extract(r_cells, df = TRUE) %&gt;% set_names(c(&quot;id&quot;, &quot;landcover&quot;)) # calculate the percent for each landcover class pland_pred &lt;- lc_extract_pred %&gt;% count(id, landcover) %&gt;% group_by(id) %&gt;% mutate(pland = n / sum(n)) %&gt;% ungroup() %&gt;% select(-n) %&gt;% # remove NAs after tallying so pland is relative to total number of cells filter(!is.na(landcover)) # convert names to be more descriptive pland_pred &lt;- pland_pred %&gt;% inner_join(lc_names, by = &quot;landcover&quot;) %&gt;% arrange(landcover) %&gt;% select(-landcover) # tranform to wide format, filling in implicit missing values with 0s pland_pred &lt;- pland_pred %&gt;% pivot_wider(names_from = lc_name, values_from = pland, values_fill = list(pland = 0)) %&gt;% mutate(year = max_lc_year) %&gt;% select(id, year, everything()) # join in coordinates pland_coords &lt;- st_transform(r_centers, crs = 4326) %&gt;% st_coordinates() %&gt;% as.data.frame() %&gt;% cbind(id = r_centers$id, .) %&gt;% rename(longitude = X, latitude = Y) %&gt;% inner_join(pland_pred, by = &quot;id&quot;) Keeping these data in a data frame is a compact way to store them and will be required once we make model predictions in later chapters. However, we can always use the raster template to convert these PLAND metrics into a spatial format, for example, if we want to map them. Let’s look at how this works for land cover class 4: deciduous broadleaf forest. forest_cover &lt;- pland_coords %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = projection(r)) %&gt;% # rasterize points rasterize(r, field = &quot;pland_04_deciduous_broadleaf&quot;) %&gt;% # project to albers equal-area for mapping projectRaster(crs = st_crs(102003)$proj4string, method = &quot;ngb&quot;) %&gt;% # trim off empty edges of raster trim() # make a map par(mar = c(0.25, 0.25, 2, 0.25)) t &lt;- str_glue(&quot;Proportion of Deciduous Broadleaf Forest\\n&quot;, &quot;{max_lc_year} MODIS Landcover&quot;) plot(forest_cover, axes = FALSE, box = FALSE, col = viridis(10), main = t) 3.5 Elevation In some scenarios, you may want to include additional covariates to complement the land cover variables. There is a wealth of open access raster data available for this purpose; however, in most cases, these data will not have a simple R interface for accessing them. Instead, you’ll typically have to manually download and process these data. As an example of how this works, we’ll demonstrate how to include covariates for elevation, which frequently plays an important role in shaping species distributions. Amatulli et al. (2018) provide a suite of global, 1km resolution topographic variables designed for use in distribution modeling. A range of variables are available, including elevation, slope, roughness, and many others; we’ll focus on elevation here, but the approach can easily be applied to other variables. To start, visit the website for these data, download the 1 km resolution median elevation product, and save the file (elevation_1KMmd_GMTEDmd.tif) in the data/ subdirectory of your project: Next we’ll load the file, crop it down from it’s full global extent to just the portion we need for BCR 27, and reproject it to the MODIS sinusoidal projection. elev &lt;- raster(&quot;data/elevation_1KMmd_GMTEDmd.tif&quot;) # crop, buffer bcr by 10 km to provide a little wiggly room elev &lt;- bcr %&gt;% st_buffer(dist = 10000) %&gt;% st_transform(crs = projection(elev)) %&gt;% crop(elev, .) %&gt;% projectRaster(crs = projection(landcover)) Now we extract the elevation values within the neighborhood of each checklist location just as we did before for the land cover data. Then we’ll calculate the median and standard deviation of the elevation within each neighborhood. # buffer each checklist location ebird_buff_noyear &lt;- ebird %&gt;% distinct(locality_id, latitude, longitude) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = projection(elev)) %&gt;% st_buffer(dist = neighborhood_radius) # extract using velox and calculate median and sd locs &lt;- st_set_geometry(ebird_buff_noyear, NULL) %&gt;% mutate(id = row_number()) elev_checklists &lt;- velox(elev)$extract(ebird_buff_noyear, df = TRUE) %&gt;% # velox doesn&#39;t properly name columns, fix that set_names(c(&quot;id&quot;, &quot;elevation&quot;)) %&gt;% # join to lookup table to get locality_id inner_join(locs, ., by = &quot;id&quot;) %&gt;% # summarize group_by(locality_id) %&gt;% summarize(elevation_median = median(elevation, na.rm = TRUE), elevation_sd = sd(elevation, na.rm = TRUE)) We’ll need to repeat this process to calculate the elevation covariates for the prediction surface. # extract using velox and calculate median and sd elev_pred &lt;- velox(elev)$extract(r_cells, df = TRUE) %&gt;% # velox doesn&#39;t properly name columns, fix that set_names(c(&quot;id&quot;, &quot;elevation&quot;)) %&gt;% # summarize group_by(id) %&gt;% summarize(elevation_median = median(elevation, na.rm = TRUE), elevation_sd = sd(elevation, na.rm = TRUE)) Finally, we’ll combine these elevation covariates with the land cover covariates. # checklist covariates pland_elev_checklist &lt;- inner_join(pland, elev_checklists, by = &quot;locality_id&quot;) write_csv(pland_elev_checklist, &quot;data/pland-elev_location-year.csv&quot;) # prediction surface covariates pland_elev_pred &lt;- inner_join(pland_coords, elev_pred, by = &quot;id&quot;) write_csv(pland_elev_pred, &quot;data/pland-elev_prediction-surface.csv&quot;) glimpse(pland_elev_pred) #&gt; Observations: 90,949 #&gt; Variables: 22 #&gt; $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … #&gt; $ longitude &lt;dbl&gt; -77.3, -77.3, -77.3, -77.3, -77.3, -77.2, -77.4, -7… #&gt; $ latitude &lt;dbl&gt; 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.… #&gt; $ year &lt;int&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 201… #&gt; $ pland_00_water &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ pland_01_evergreen_needleleaf &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0… #&gt; $ pland_02_evergreen_broadleaf &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ pland_03_deciduous_needleleaf &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ pland_04_deciduous_broadleaf &lt;dbl&gt; 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0… #&gt; $ pland_05_mixed_forest &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.1905, 0.0952, 0.2857, 0.0… #&gt; $ pland_06_closed_shrubland &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ pland_07_open_shrubland &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ pland_08_woody_savanna &lt;dbl&gt; 0.952, 0.857, 0.810, 0.619, 0.905, 0.714, 0.619, 1.… #&gt; $ pland_09_savanna &lt;dbl&gt; 0.0000, 0.0000, 0.1905, 0.1905, 0.0000, 0.0000, 0.0… #&gt; $ pland_10_grassland &lt;dbl&gt; 0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0… #&gt; $ pland_11_wetland &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ pland_12_cropland &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0… #&gt; $ pland_13_urban &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3… #&gt; $ pland_14_mosiac &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0… #&gt; $ pland_15_barren &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ elevation_median &lt;dbl&gt; 40.7, 41.8, 42.5, 43.2, 42.6, 44.3, 45.5, 44.4, 43.… #&gt; $ elevation_sd &lt;dbl&gt; 4.347, 2.135, 3.059, 3.373, 1.331, 1.717, 2.020, 3.… This completes the data preparation. The following chapters will focus on using these data to model species distributions. References "],
["encounter.html", "Chapter 4 Modeling Encounter Rate 4.1 Introduction 4.2 Data preparation 4.3 Spatiotemporal subsampling 4.4 Random forests 4.5 Habitat associations 4.6 Prediction", " Chapter 4 Modeling Encounter Rate 4.1 Introduction In this chapter we’ll estimate the encounter rate of Wood Thrush on eBird checklists in June in BCR 27. We define encounter rate as measuring the probability of an eBirder encountering a species on a standard eBird checklist. The ecological metric we’re ultimately interested in is the probability that a species occurs at a site (i.e. the occupancy probability). This is usually not possible to estimate with semi-structured citizen science data like those from eBird because we typically can’t estimate absolute detectability. However, by accounting for much of the variation in detectability by including effort covariates in our model, the remaining unaccounted detectability will be more consistent across sites (Guillera-Arroita et al. 2015). Therefore, the encounter rate metric will be proportional to occupancy, albeit lower by some consistent amount. For some easily detectable species the difference between occurrence and actual occupancy rate will be small, and these encounter rates will approximate the actual occupancy rates of the species. For harder to detect species, the encounter rate may be substantially lower than the occupancy rate. Random forests are a general purpose machine learning method applicable to a wide range of classification and regression problems, including the task at hand: classifying detection and non-detection of a species on eBird checklists. In addition to having good predictive performance, random forests are reasonably easy to use and have several efficient implementations in R. Prior to fitting a random forest model, we’ll demonstrate how to address issues of class imbalance and spatial bias using spatial subsampling on a regular grid. After fitting the model, we’ll assess its performance using a subset of data put aside for testing, and calibrate the model to ensure predictions are accurate. Finally, we’ll predict encounter rates throughout the study area and produce maps of these predictions. 4.2 Data preparation Let’s get started by loading the necessary packages and data. If you worked through the previous chapters, you should have all the data required for this chapter. However, you may want to download the data package, and unzip it to your project directory, to ensure you’re working with exactly the same data as was used in the creation of this book. library(sf) library(raster) library(dggridR) library(lubridate) library(ranger) library(scam) library(PresenceAbsence) library(verification) library(edarf) library(ebirdst) library(fields) library(gridExtra) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select projection &lt;- raster::projection map &lt;- purrr::map # set random number seed to insure fully repeatable results set.seed(1) # setup output directory for saved results if (!dir.exists(&quot;output&quot;)) { dir.create(&quot;output&quot;) } # ebird data ebird &lt;- read_csv(&quot;data/ebd_woothr_june_bcr27_zf.csv&quot;) %&gt;% # year required to join to habitat data mutate(year = year(observation_date)) # modis habitat covariates habitat &lt;- read_csv(&quot;data/pland-elev_location-year.csv&quot;) %&gt;% mutate(year = as.integer(year)) # combine ebird and habitat data ebird_habitat &lt;- inner_join(ebird, habitat, by = c(&quot;locality_id&quot;, &quot;year&quot;)) # prediction surface pred_surface &lt;- read_csv(&quot;data/pland-elev_prediction-surface.csv&quot;) # latest year of landcover data max_lc_year &lt;- max(pred_surface$year) r &lt;- raster(&quot;data/prediction-surface.tif&quot;) # load gis data for making maps map_proj &lt;- st_crs(102003) ne_land &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_land&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() bcr &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;bcr&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_country_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_country_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_state_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_state_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() 4.3 Spatiotemporal subsampling As discussed in the introduction, three of the challenges faced when using eBird data, are spatial bias, temporal bias, and class imbalance. Spatial and temporal bias refers to the tendency of eBird checklists to be distributed non-randomly in space and time, while class imbalance refers to fact that there will be many more non-detections than detections for most species. All three can impact our ability to make reliable inferences from these data. Fortunately, all three can largely be addressed through subsampling the eBird data prior to modeling. In particular, we define an equal area hexagonal grid across the study region, and then subsample detections and non-detections separately to ensure that we don’t lose too many detections. In our example, we will be sampling one detection and one non-detection checklist from each grid cell for each week. Hexagonal grids may seem exotic relative to square grids, which may be more familiar; however, they have a variety of benefits (Sahr 2011) including significantly less spatial distortion. With hexagonal grids we can be sure all the cells are of equal area, which is particularly important if we have a large region for modeling. The R package dggridR makes working with hexagonal grids simple and efficient. We’ll construct a grid with 5 km spacing between the centres of adjacent hexagons, then sample randomly from these hexagonal cells. Before working with the real data, it’s instructive to look at a simple toy example, to see how this subsampling process works. We’ll generate a few hundred random points, overlay a hexagonal grid, then sample one point from each cell. # bounding box to generate points from bb &lt;- st_bbox(c(xmin = -0.1, xmax = 0.1, ymin = -0.1, ymax = 0.1), crs = 4326) %&gt;% st_as_sfc() %&gt;% st_sf() # random points pts &lt;- st_sample(bb, 500) %&gt;% st_sf(as.data.frame(st_coordinates(.)), geometry = .) %&gt;% rename(lat = Y, lon = X) # contruct a hexagonal grid with ~ 5 km between cells dggs &lt;- dgconstruct(spacing = 5) # for each point, get the grid cell pts$cell &lt;- dgGEO_to_SEQNUM(dggs, pts$lon, pts$lat)$seqnum # sample one checklist per grid cell pts_ss &lt;- pts %&gt;% group_by(cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() # generate polygons for the grid cells hexagons &lt;- dgcellstogrid(dggs, unique(pts$cell), frame = FALSE) %&gt;% st_as_sf() ggplot() + geom_sf(data = hexagons) + geom_sf(data = pts, size = 0.5) + geom_sf(data = pts_ss, col = &quot;red&quot;) + theme_bw() Now let’s apply exactly the same approach to subsampling the real eBird checklists; however, now we subsample temporally in addition to spatially, and sample detections and non-detections separately. # generate hexagonal grid with ~ 5 km betweeen cells dggs &lt;- dgconstruct(spacing = 5) # get hexagonal cell id and week number for each checklist checklist_cell &lt;- ebird_habitat %&gt;% mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum, year = year(observation_date), week = week(observation_date)) # sample one checklist per grid cell per week # sample detection/non-detection independently ebird_ss &lt;- checklist_cell %&gt;% group_by(species_observed, year, week, cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() How did this impact the prevalence of detections compared to non-detections? # original data nrow(ebird_habitat) #&gt; [1] 48445 count(ebird_habitat, species_observed) %&gt;% mutate(percent = n / sum(n)) #&gt; # A tibble: 2 x 3 #&gt; species_observed n percent #&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 FALSE 46350 0.957 #&gt; 2 TRUE 2095 0.0432 # after sampling nrow(ebird_ss) #&gt; [1] 20090 count(ebird_ss, species_observed) %&gt;% mutate(percent = n / sum(n)) #&gt; # A tibble: 2 x 3 #&gt; species_observed n percent #&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 FALSE 18614 0.927 #&gt; 2 TRUE 1476 0.0735 So, the subsampling decreased the overall number of checklists by a factor of about four, but increased the prevalence of detections from 4% to 7%. This increase in detections will help the random forest model distinguish where birds are being observed; however, this does affect the prevalence rate of the detections in the data. As a result, the estimated probability of occurrence based on these subsampled data will be larger than the true occurrence rate. When examining the outputs from the models it will be important to recall that we altered the prevalence rate at this stage. Now let’s look at how the subsampling affects the spatial distribution of the observations. # convert checklists to spatial features all_pts &lt;- ebird_habitat %&gt;% st_as_sf(coords = c(&quot;longitude&quot;,&quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = map_proj) %&gt;% select(species_observed) ss_pts &lt;- ebird_ss %&gt;% st_as_sf(coords = c(&quot;longitude&quot;,&quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = map_proj) %&gt;% select(species_observed) both_pts &lt;- list(before_ss = all_pts, after_ss = ss_pts) # map p &lt;- par(mfrow = c(2, 1)) for (i in seq_along(both_pts)) { par(mar = c(0.25, 0.25, 0.25, 0.25)) # set up plot area plot(st_geometry(both_pts[[i]]), col = NA) # contextual gis data plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) plot(bcr, col = &quot;#cccccc&quot;, border = NA, add = TRUE) plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) plot(ne_country_lines, col = &quot;#ffffff&quot;, lwd = 1.5, add = TRUE) # ebird observations # not observed plot(st_geometry(both_pts[[i]]), pch = 19, cex = 0.1, col = alpha(&quot;#555555&quot;, 0.25), add = TRUE) # observed plot(filter(both_pts[[i]], species_observed) %&gt;% st_geometry(), pch = 19, cex = 0.3, col = alpha(&quot;#4daf4a&quot;, 0.5), add = TRUE) # legend legend(&quot;bottomright&quot;, bty = &quot;n&quot;, col = c(&quot;#555555&quot;, &quot;#4daf4a&quot;), legend = c(&quot;Non-detection&quot;, &quot;Detection&quot;), pch = 19) box() par(new = TRUE, mar = c(0, 0, 3, 0)) if (names(both_pts)[i] == &quot;before_ss&quot;) { title(&quot;Wood Thrush eBird Observations\\nBefore subsampling&quot;) } else { title(&quot;After subsampling&quot;) } } par(p) For Wood Thrush, subsampling the detections and non-detections independently is sufficient for dealing with class imbalance. You can assess the impact of class imbalance by looking at the prevalence rates and examining whether the models are good at predicting to validation data. For species that are extremely rare, it may be worthwhile considering keeping all detections or even oversampling detections (Robinson, Ruiz‐Gutierrez, and Fink 2018). In doing this, be aware that some of your species detections will not be independent, which could lead to overfitting of the data. Overall, when thinking about the number of detections and the prevalence rate, it’s important to consider both the ecology and detectability of the focal species, and the behavior of observers towards this species. 4.4 Random forests Now we’ll use a random forest model to relate detection/non-detection of Wood Thrush to the habitat covariates (MODIS land cover and elevation), while also accounting for variation in detectability by including a suite of effort covariates. Before we fit the random forest model, we randomly split the data into 80% of checklists for training and 20% for testing. We’ll hold this 20% aside when we fit the model, then use it as an independent data set to test the predictive performance of the model. ebird_split &lt;- ebird_ss %&gt;% # select only the columns to be used in the model select(species_observed, year, day_of_year, time_observations_started, duration_minutes, effort_distance_km, number_observers, starts_with(&quot;pland_&quot;), starts_with(&quot;elevation_&quot;)) %&gt;% drop_na() # split 80/20 ebird_split &lt;- ebird_split %&gt;% split(if_else(runif(nrow(.)) &lt;= 0.8, &quot;train&quot;, &quot;test&quot;)) map_int(ebird_split, nrow) #&gt; test train #&gt; 3933 16125 Although we were able to partially address the issue of class imbalance via subsampling, detections still only make up 7% of observations, and for rare species this number will be even lower. Most classification algorithms aim to minimize the overall error rate, which results in poor predictive performance for rare classes (Chen, Liaw, and Breiman 2004). To address this issue, we’ll use a balanced random forest approach, a modification of the traditional random forest algorithm designed to handle imbalanced data. In this approach, each of the trees that makes up the random forest is generated using a random sample of the data chosen such that there is an equal number of the detections (the rare class) and non-detections (the common class). To use this approach, we’ll need to calculate the proportion of detections in the dataset. detection_freq &lt;- mean(ebird_split$train$species_observed) There are several packages for fitting random forests in R; however, we’ll use ranger, which is a blazingly fast implementation with all the features we need. To fit a balanced random forest, we use the sample.fraction parameter to instruct ranger to grow each tree based on a random sample of the data that has an equal number of detections and non-detections. Specifying this is somewhat obtuse, because we need to tell ranger the proportion of the total data set to sample for non-detections and detections, and when this proportion is the same as the proportion of the rarer class–the detections–then then ranger will sample from all of the rarer class but from an equally sized subset of the more common non-detections. We use replace = TRUE to ensure that it’s a bootstrap sample. We’ll also ask ranger to predict probabilities, rather than simply returning the most probable class, with probability = TRUE. # ranger requires a factor response to do classification ebird_split$train$species_observed &lt;- factor(ebird_split$train$species_observed) # grow random forest rf &lt;- ranger(formula = species_observed ~ ., data = ebird_split$train, importance = &quot;impurity&quot;, probability = TRUE, replace = TRUE, sample.fraction = c(detection_freq, detection_freq)) 4.4.1 Calibration For various reasons, the predicted probabilities from models do not always align with the observed frequencies of detections. For example, we would hope that if we look at all sites with a estimated probability of encounter of 0.2, that 20% of these would record the species. However, these probabilities are not always so well aligned. This will clearly be the case in our example, because we have deliberately inflated the prevalence of detection records in the data through the spatiotemporal subsampling process. We can produce a calibration of the predictions, which can be a useful diagnostic tool to understand the model predictions, and in some cases can be used to realign the predictions with observations. For information on calibration in species distribution models see Vaughan and Ormerod (2005) and for more fundamental references on calibration see Platt (1999), Murphy (1973), and Niculescu-Mizil and Caruana (2005). To view the calibration of our model results, we predict encounter rate for each checklist in the training set, then fit a binomial Generalized Additive Model (GAM) with the real observed encounter rate as the response and the predicted encounter rate as the predictor variable. Whereas GLMs fit a linear relationship between a response and predictors, GAMs allow non-linear relationships. Although GAMs provide a degree of flexibility, however in some situations they may overfit and provide unrealistic and unhelpful calibrations. We have a strong a priori expectation that higher real values will also be associated with higher estimated rates. In order to maintain the ranking of predictions, it is important that we respect this ordering and to do this we’ll use a GAM that is constrained to only increase. To fit the GAM, we’ll use the R package scam, so the shape can be constrained to be monotonically increasing. Note that predictions from ranger are in the form of a matrix of probabilities for each class, and we want the probability of detections, which is the second column of this matrix. # make predictions on training data occ_pred &lt;- rf$predictions[, 2] # convert the observered response back to a numeric value from factor occ_obs &lt;- ebird_split$train$species_observed %&gt;% as.logical() %&gt;% as.integer() rf_pred_train &lt;- tibble(obs = occ_obs, pred = occ_pred) %&gt;% drop_na() # fit calibration model calibration_model &lt;- scam(obs ~ s(pred, k = 5, bs = &quot;mpi&quot;), gamma = 1.4, data = rf_pred_train) # calculate the average observed encounter rates for different # categories of estimated encounter rates average_encounter &lt;- rf_pred_train %&gt;% mutate(pred_cat = cut(rf_pred_train$pred, breaks = seq(0, 1, by=0.02))) %&gt;% group_by(pred_cat) %&gt;% summarise(pred = mean(pred), obs = mean(obs), checklist_count = n()) %&gt;% ungroup() # plot cal_pred &lt;- tibble(pred = seq(0, 1, length.out = 100)) cal_pred &lt;- predict(calibration_model, cal_pred, type = &quot;response&quot;) %&gt;% bind_cols(cal_pred, calibrated = .) ggplot(cal_pred) + aes(x = pred, y = calibrated) + geom_line() + geom_point(data = average_encounter, aes(x = pred, y = obs, size = sqrt(checklist_count)), show.legend = FALSE, shape = 1) + labs(x = &quot;Estimated encounter rate&quot;, y = &quot;Observed encounter rate&quot;, title = &quot;Calibration model&quot;) Using this as a diagnostic tool, we can clearly see that the estimated encounter rates are mostly much larger than the observed encounter rates. So we see that the model is not well calibrated. However, we do see from the points that the relative ranking of predictions is largely good. Sites with estimated higher encounter rate do mostly have higher encounter rates. From this we have learnt that the model is good at distinguishing sites with high rates from those with low rates. For those readers familiar with using AUC scores to assess the quality of species distribution models, the graph is telling us that the model should have a high AUC value. However, the model is not so good at estimating encounter rates accurately. If accurate encounter rates are required, and the calibration model is strong (close fit of points to the line in the figure above), then the calibration model can be used to calibrate the estimates from the random forest model, so they are adjusted to match the observed encounter rates more closely. The calibrated random forest model is the combination of the original random forest model followed by the calibration model. If you’re using this model to calibrate your estimates, notice that the calibration curve can produce probabilities greater than 1 and less than 0, so when applying the calibration we also need to restrict the predictions to be between 0 and 1. It’s possible to run a logistic regression for the calibration to remove these predictions less than 0 or greater than 1; however, we’ve found the Gaussian constrained GAM to be more stable than the logistic constrained GAM. 4.4.2 Assessment To assess the quality of both the uncalibrated and the calibrated model, we’ll validate the model’s ability to predict the observed patterns of occupancy using independent validation data (i.e. the 20% test data set). We’ll use a range of predictive performance metrics to compare the predictions to the actual observations: mean squared error (MSE), sensitivity, specificity, AUC, and Kappa. Several of these metrics require the predicted probabilities to be classified into detection/non-detection. We’ll do this using a threshold, chosen to maximize the Kappa statistic. # predict on test data using calibrated model p_fitted &lt;- predict(rf, data = ebird_split$test, type = &quot;response&quot;) # extract probability of detection p_fitted &lt;- p_fitted$predictions[, 2] # calibrate p_calibrated &lt;- predict(calibration_model, newdata = tibble(pred = p_fitted), type = &quot;response&quot;) rf_pred_test &lt;- data.frame(id = seq_along(p_calibrated), # actual detection/non-detection obs = ebird_split$test$species_observed, # uncalibrated prediction fit = p_fitted, # calibrated prediction cal = p_calibrated) %&gt;% # constrain probabilities to 0-1 mutate(cal = pmin(pmax(cal, 0), 1)) %&gt;% drop_na() # mean squared error (mse) mse_fit &lt;- mean((rf_pred_test$obs - rf_pred_test$fit)^2, na.rm = TRUE) mse_cal &lt;- mean((rf_pred_test$obs - rf_pred_test$cal)^2, na.rm = TRUE) # pick threshold to maximize kappa opt_thresh &lt;- optimal.thresholds(rf_pred_test, opt.methods = &quot;MaxKappa&quot;) # calculate accuracy metrics: auc, kappa, sensitivity, specificity, metrics_fit &lt;- rf_pred_test %&gt;% select(id, obs, fit) %&gt;% presence.absence.accuracy(threshold = opt_thresh$fit, na.rm = TRUE, st.dev = FALSE) metrics_cal &lt;- rf_pred_test %&gt;% select(id, obs, cal) %&gt;% presence.absence.accuracy(threshold = opt_thresh$cal, na.rm = TRUE, st.dev = FALSE) rf_assessment &lt;- tibble( model = c(&quot;RF&quot;, &quot;Calibrated RF&quot;), mse = c(mse_fit, mse_cal), sensitivity = c(metrics_fit$sensitivity, metrics_cal$sensitivity), specificity = c(metrics_fit$specificity, metrics_cal$specificity), auc = c(metrics_fit$AUC, metrics_cal$AUC), kappa = c(metrics_fit$Kappa, metrics_cal$Kappa) ) knitr::kable(rf_assessment, digits = 3) model mse sensitivity specificity auc kappa RF 0.143 0.563 0.892 0.849 0.333 Calibrated RF 0.062 0.563 0.892 0.849 0.333 Each of these metrics can inform us about different aspects of the model fit. The objectives of your study will determine which of these metrics is most important. For example, if you want to ensure that the model definitely includes all areas of species presence, you would seek to have high sensitivity. Alternatively, if you want to ensure that places the model predicts for species occurrence are true (for example, when identifying areas for conservation action), you would seek to maximise specificity. Note that in our example, calibration had little effect on any of the metrics of accurate classification (i.e. binary presence/absence), but the mean squared error in estimation of probability of occurrence was halved by calibration. 4.5 Habitat associations From the random forest model, we can glean two important sources of information about the association between Wood Thrush detection and features of their local environment. First, predictor importance is a measure of the predictive power of each covariate, and is calculated as a byproduct of fitting a random forest model. Second, partial dependence plots estimate the marginal effect of one predictor holding all other predictors constant. 4.5.1 Predictor importance During the process of fitting a random forest model, some variables are removed at each node of the trees that make up the random forest. Predictor importance is based on the mean decrease in accuracy of the model when a given covariate is not used. It’s technically an average Gini index, but essentially larger values indicate that a predictor is more important to the model. pi &lt;- enframe(rf$variable.importance, &quot;predictor&quot;, &quot;importance&quot;) # plot ggplot(pi) + aes(x = fct_reorder(predictor, importance), y = importance) + geom_col() + geom_hline(yintercept = 0, size = 2, colour = &quot;#555555&quot;) + scale_y_continuous(expand = c(0, 0)) + coord_flip() + labs(x = NULL, y = &quot;Predictor Importance (Gini Index)&quot;) + theme_minimal() + theme(panel.grid = element_blank(), panel.grid.major.x = element_line(colour = &quot;#cccccc&quot;, size = 0.5)) The most important predictors of detection/non-detection are generally effort variables. Indeed, that’s the case here: start time and checklist duration both have high predictor importance. This tells us that the time of day and length of time the observer was out, both have a large effect on whether Wood Thrush was recorded on the checklist. Both elevation covariates have high importance, and the top descriptors of vegetation type are the proportions of mixed forest, woody savanna, and deciduous broadleaf forest. Note that high importance doesn’t tell us the direction of the relationship with detection, for that we’ll have to look at partial dependence plots. 4.5.2 Partial dependence Partial dependence plots show the marginal effect of a given predictor on encounter rate averaged across the other predictors. These plots are generated by predicting encounter rate at a regular sequence of points across the full range of values of a given predictor. At each predictor value, predictions of encounter rate are made for a random subsample of the training dataset with the focal predictor fixed, but all other predictors left as is. The encounter rate predictions are then averaged across all the checklists in the training dataset giving an estimate of the average encounter rate at a specific value of the focal predictor. This is a cumbersome process, but fortunately the R package edarf has a function to calculate partial dependence from random forest models. Let’s look at partial dependence plots for the most important predictors. # top 9 predictors other than date top_pred &lt;- pi %&gt;% filter(!predictor %in% c(&quot;year&quot;, &quot;day_of_year&quot;)) %&gt;% top_n(n = 9, wt = importance) %&gt;% arrange(desc(importance)) # calculate partial dependence for each predictor # map is used to iteratively apply partial_dependence to each predictor pd &lt;- top_pred %&gt;% mutate(pd = map(predictor, partial_dependence, fit = rf, data = ebird_split$train, n = c(25, 1000)), pd = map(pd, ~ .[, c(1, 3)]), pd = map(pd, set_names, nm = c(&quot;value&quot;, &quot;encounter_rate&quot;))) %&gt;% unnest(cols = pd) # calibrate predictions pd$encounter_rate &lt;- predict(calibration_model, newdata = tibble(pred = pd$encounter_rate), type = &quot;response&quot;) %&gt;% as.numeric() # constrain probabilities to 0-1 pd$encounter_rate &lt;- pmin(pmax(pd$encounter_rate, 0), 1) # plot ggplot(pd) + aes(x = value, y = encounter_rate) + geom_line() + geom_point() + scale_y_continuous(labels = scales::percent) + facet_wrap(~ as_factor(predictor), nrow = 3, scales = &quot;free&quot;) + labs(x = NULL, y = &quot;Encounter Rate&quot;) + theme_minimal() + theme_minimal() + theme(panel.grid = element_blank(), axis.line = element_line(color = &quot;grey60&quot;), axis.ticks = element_line(color = &quot;grey60&quot;)) There are a range of interesting responses here. As seen in Section 2.5, the encounter rate for Wood Thrush peaks early in the morning when they’re most likely to be singing, then quickly drops off in the middle of the day, before slightly increasing in the evening. Some other predictors show a more smoothly increasing relationship with encounter rate, for example, as the landscape contains more deciduous forest, the encounter rate increases. The random forest model has a number of interactions, which are not displayed in these partial dependence plots. When interpreting these, bear in mind that there are likely some more complex interaction effects beneath these individual plots. 4.6 Prediction Now for the fun part: let’s use the calibrated random forest model to make a map of Wood Thrush encounter rate in BCR 27! In Section 3.4, we created a prediction surface consisting of the PLAND habitat covariates summarized on a regular grid of points across BCR 27. In this section, we’ll make predictions of encounter rate at these points. However, first we need to bring effort variables into this prediction surface. We’ll make predictions for a standard eBird checklist: a 1 km, 1 hour traveling count at the peak time of day for detecting this species. Finally, we’ll make these predictions for June 15, 2018, the middle of our June focal window for the latest year for which MODIS landcover data exist. To find the time of day with the highest detection probability, we can look for the peak of the partial dependence plot. The one caveat to this approach is that it’s important we focus on times of day for which there are enough data to make predictions. In particular, there’s an increasing trend in detectability with earlier start times, and few checklists late at night, which can cause the model to incorrectly extrapolate that trend to show highest detectability at night. Let’s start by looking at a plot to see if this is happening here. # find peak time of day from partial dependence pd_time &lt;- partial_dependence(rf, vars = &quot;time_observations_started&quot;, # make estimates at 30 minute intervals # using a subset of the training dataset n = c(24 * 2, 1000), data = ebird_split$train) %&gt;% select(time_observations_started, encounter_rate = &quot;TRUE&quot;) # histogram g_hist &lt;- ggplot(ebird_split$train) + aes(x = time_observations_started) + geom_histogram(binwidth = 1, center = 0.5, color = &quot;grey30&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(0, 24, by = 3)) + scale_y_continuous(labels = scales::comma) + labs(x = &quot;Hours since midnight&quot;, y = &quot;# checklists&quot;, title = &quot;Distribution of observation start times&quot;) # gam g_pd &lt;- ggplot(pd_time) + aes(x = time_observations_started, y = encounter_rate) + geom_line() + scale_x_continuous(breaks = seq(0, 24, by = 3)) + labs(x = &quot;Hours since midnight&quot;, y = &quot;Probability of reporting&quot;, title = &quot;Observation start time partial dependence&quot;) # combine grid.arrange(g_hist, g_pd) The peak probability of reporting is very close to the time of day during which the abundance of reports starts to increase, but from these graphs it is not entirely clear that the early morning peak in reports is well substantiated by abundant data. Let’s instead look for the peak time within hours of the day that contain at least 1% of the training data. # hours with at least 1% of checklists search_hours &lt;- ebird_split$train %&gt;% mutate(hour = floor(time_observations_started)) %&gt;% count(hour) %&gt;% mutate(pct = n / sum(n)) %&gt;% filter(pct &gt;= 0.01) # constrained peak time t_peak &lt;- pd_time %&gt;% filter(floor(time_observations_started) %in% search_hours$hour) %&gt;% top_n(1, wt = desc(time_observations_started)) %&gt;% pull(time_observations_started) t_peak #&gt; [1] 5.07 Based on this analysis, the best time for detecting Wood Thrush is at 5:04 AM. Now we use this time to make predictions. This is equivalent to many eBirders all conducting a checklist within different grid cells on June 15 at 5:04 AM. We also add the other effort variables to the prediction dataset. # add effort covariates to prediction pred_surface_eff &lt;- pred_surface %&gt;% mutate(observation_date = ymd(str_glue(&quot;{max_lc_year}-06-15&quot;)), year = year(observation_date), day_of_year = yday(observation_date), time_observations_started = t_peak, duration_minutes = 60, effort_distance_km = 1, number_observers = 1) # predict pred_rf &lt;- predict(rf, data = pred_surface_eff, type = &quot;response&quot;) pred_rf &lt;- pred_rf$predictions[, 2] # apply calibration models pred_rf_cal &lt;- predict(calibration_model, data.frame(pred = pred_rf), type = &quot;response&quot;) # add to prediction surface pred_er &lt;- bind_cols(pred_surface_eff, encounter_rate = pred_rf_cal) %&gt;% select(latitude, longitude, encounter_rate) %&gt;% mutate(encounter_rate = pmin(pmax(encounter_rate, 0), 1)) Next, we’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction surface raster template. r_pred &lt;- pred_er %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = projection(r)) %&gt;% # rasterize rasterize(r) r_pred &lt;- r_pred[[-1]] # save the raster tif_dir &lt;- &quot;output&quot; if (!dir.exists(tif_dir)) { dir.create(tif_dir) } writeRaster(r_pred, file.path(tif_dir, &quot;rf-model_encounter-rate_woothr.tif&quot;), overwrite = TRUE) Finally, we can map these predictions! # project predictions r_pred_proj &lt;- projectRaster(r_pred, crs = map_proj$proj4string, method = &quot;ngb&quot;) par(mar = c(3.5, 0.25, 0.25, 0.25)) # set up plot area plot(bcr, col = NA, border = NA) plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) # encounter rate r_max &lt;- ceiling(10 * cellStats(r_pred_proj, max)) / 10 brks &lt;- seq(0, r_max, by = 0.025) lbl_brks &lt;- seq(0, r_max, by = 0.1) # ebird status and trends color palette pal &lt;- abundance_palette(length(brks) - 1) plot(r_pred_proj, col = pal, breaks = brks, maxpixels = ncell(r_pred_proj), legend = FALSE, add = TRUE) # borders plot(bcr, border = &quot;#000000&quot;, col = NA, lwd = 1, add = TRUE) plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) plot(ne_country_lines, col = &quot;#ffffff&quot;, lwd = 1.5, add = TRUE) box() # legend par(new = TRUE, mar = c(0, 0, 0, 0)) title &lt;- &quot;Wood Thrush Encounter Rate&quot; image.plot(zlim = range(brks), legend.only = TRUE, col = pal, breaks = brks, smallplot = c(0.25, 0.75, 0.06, 0.09), horizontal = TRUE, axis.args = list(at = lbl_brks, labels = lbl_brks, fg = &quot;black&quot;, col.axis = &quot;black&quot;, cex.axis = 0.75, lwd.ticks = 0.5, padj = -1.5), legend.args = list(text = title, side = 3, col = &quot;black&quot;, cex = 1, line = 0)) References "],
["occupancy.html", "Chapter 5 Modeling Occupancy 5.1 Introduction 5.2 Data preparation 5.3 Occupancy modeling 5.4 Prediction", " Chapter 5 Modeling Occupancy 5.1 Introduction In this chapter, we’ll cover the basic steps for applying occupancy models to eBird data. In Chapter 4, we used analytical approaches that accounted for variation in detectability. We included covariates that are known to influence detectability (e.g. duration, time of day) that we incorporated into the model alongside the covariates that influence occurrence. In contrast, occupancy models separate the ecological process of species occurrence and the observation process of species detection. These two processes are modeled together, but estimated as separate processes. This modeling framework allows us to account for detectabilty when estimating species occurrence. In this book, we won’t delve into the mechanics of occupancy models; however, there is a wealth of background literature on occupancy modeling and readers wishing to learn more about this field may want to consult the book on the topic by MacKenzie et al. (2017). The application of these models typically requires data from repeated sampling visits (occasions) to a single site during a time frame over which there is appearance or disappearance of birds from the site (the population is closed). Although eBird checklists do not naturally meet these requirements, it is possible to apply occupancy models to eBird data by extracting a subset of the data that do conform to a repeat-sampling data structure. Here, we present a simple example of how to process eBird data to meet the assumptions of occupancy models. To illustrate our example, we apply a single-season occupancy model to estimate occupancy and detection probabilities for Wood Thrush in the month of June for BCR 27. This chapter differs from the previous chapter on modeling encounter rate in two important ways. First, the random forest model used in Chapter 4 is an example of a machine learning approach, while the occupancy models used in this chapter use a more traditional likelihood approach. This latter class of statistical models are widely used for addressing specific questions and hypotheses, while the goal of machine learning is primarily to identify patterns and make predictions (Bzdok, Altman, and Krzywinski 2018). Second, machine learning approaches can accommodate complex non-linear effects and interactions between covariates, and are useful when modeling habitat associations that can vary across large spatial and temporal scales. In contrast, occupancy models are well suited for describing linear effects and simpler interactions. In this example, we specifically focus on the mechanics of filtering and formatting the data to fit occupancy models, and less on the specifics of model selection and how to choose suitable predictor variables for estimating detection and occupancy probabilities. The predictors we do include are informed by our inferences on variable importance scores from the random forest model in Chapter 4, as well as our existing knowledge of the species being modeled. If you worked through the previous chapters, you should have all the data necessary for this chapter. You can also download the data package, and unzip it to your project directory. library(auk) library(lubridate) library(sf) library(dggridR) library(unmarked) library(raster) library(ebirdst) library(MuMIn) library(AICcmodavg) library(fields) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select projection &lt;- raster::projection # set random number seed to insure fully repeatable results set.seed(1) # setup output directory for saved results if (!dir.exists(&quot;output&quot;)) { dir.create(&quot;output&quot;) } # ebird data ebird &lt;- read_csv(&quot;data/ebd_woothr_june_bcr27_zf.csv&quot;) %&gt;% mutate(year = year(observation_date), # occupancy modeling requires an integer response species_observed = as.integer(species_observed)) # modis land cover covariates habitat &lt;- read_csv(&quot;data/pland-elev_location-year.csv&quot;) %&gt;% mutate(year = as.integer(year)) # combine ebird and modis data ebird_habitat &lt;- inner_join(ebird, habitat, by = c(&quot;locality_id&quot;, &quot;year&quot;)) # prediction surface pred_surface &lt;- read_csv(&quot;data/pland-elev_prediction-surface.csv&quot;) # latest year of landcover data max_lc_year &lt;- pred_surface$year[1] r &lt;- raster(&quot;data/prediction-surface.tif&quot;) # load gis data for making maps map_proj &lt;- st_crs(102003) ne_land &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_land&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() bcr &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;bcr&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_country_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_country_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_state_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_state_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() 5.2 Data preparation First, we need to extract a subset of the eBird data that meets the assumptions of occupancy models, then we perform spatiotemporal subsampling to deal with spatial bias in the data. Let’s start by filtering our data to include only checklists with 5 or fewer observers, to reduce sources of variation in detectability, and because there are very few checklists with more than 5 observers. In addition, we’ll subset observations to the most recent year for which we have data (2019) to fit a single-season occupancy model. # filter prior to creating occupancy model data ebird_filtered &lt;- filter(ebird_habitat, number_observers &lt;= 5, year == max(year)) In some situations, you may want to further filter the data based on the results of an exploratory analysis similar to the one conducted in Section 2.5. However, we won’t further filter the observations in eBird for our occupancy example. Given the extra constraints for data suitable for occupancy modeling, it may be useful to retain more checklists at this stage. 5.2.1 Data formatting Next, we need to generate detection histories for each location we define as a site. In this example, we define the month of June as the time period over which we assume that the population is closed for Wood Thrush in BCR 27, and a site is defined as a specific location (latitude/longitude) that is visited at least twice by the same observer within our defined period of closure (i.e. the month of June). The auk function filter_repeat_visits() is designed to extract a subset of eBird data suitable for occupancy modeling. Using the function, we first filter the data to only sites that have at least 2 visits (min_obs). We also define the maximum number of repeat visits (max_obs) as 10 visits or checklists. When a specific site has been visited more than 10 times, the function will randomly select 10 checklists from all the visits to that site. If we had data from more than one year, we would use annual_closure = TRUE to determine that populations are closed within years, but not closed between years (i.e. species occurrence can change between years). If we want to define periods of closure within years, we can define these in terms of the number of days using n_days. For example, n_days = 10 would define contiguous sets of 10 days, starting with the earliest observation date in the data, and use these as consecutive periods of closure. Here we don’t define n_days so we are treating all the June 2019 checklists as a single season and with a closed population. Finally, site_vars specifies the set of variables that defines a site. In this example, a site is defined jointly by the location and observer IDs. Any set of variables in the data can be used to define sites. For example, site_vars = &quot;locality_id&quot; could be used to define sites using the location regardless of observer. occ &lt;- filter_repeat_visits(ebird_filtered, min_obs = 2, max_obs = 10, annual_closure = TRUE, date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;, &quot;observer_id&quot;)) # entire data set nrow(ebird_habitat) #&gt; [1] 48445 # reduced data set nrow(occ) #&gt; [1] 3724 # number of individual sites n_distinct(occ$site) #&gt; [1] 988 This function filter_repeat_visits() added three new columns to the dataset: site is a unique site ID (here, location and observer), closure_id identifies the primary period of closure (in this example the year), and n_observations is the number of visits to each site. Our capture histories are now properly formatted for occupancy models and ready to be analyzed. Note that we’ve made a trade off in sample size, dropping from 10,414 checklists to 3,724 checklists over 988 sites. We’ll use our filtered observations to fit a single-season occupancy model using the unmarked R package. For additional details on the type of data format required for this package, consult the documentation for the unmarked function formatWide(). The auk function format_unmarked_occu() converts data from a vertical format in which each row is an observation (as in the EBD) to a horizontal detection history required by unmarked, where each row is a site. At this stage, we need to specify which variables will be ecological process (i.e. occupancy) covariates and which will be observational process (i.e. detection) covariates. Occupancy covariates (site_covs) will be unique at the level of the site, while detection covariates (obs_covs) will be unique for each site and sampling occasion (i.e. checklist). For this example, we’ll use MODIS land cover variables as habitat covariates for modeling the occupancy probability of Wood Thrush. Based on predictor importance measures from Chapter 4, we include deciduous broadleaf forest and mixed forest as habitat types for which we expect positive relationships with occupancy, and croplands and urban, for which we expect negative relationships. To estimate detection probability, we include five effort variables that are related to the detection process. Habitat type has been shown to affect detectability, for example, some species are harder to detect in densely forested habitats relative to more open habitat types. So we also include deciduous broadleaf forest and mixed forest as variables in the detectability model. Occupancy models allow us to tease apart the differing effects of habitat on both detection and occupancy probabilities. # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;species_observed&quot;, site_covs = c(&quot;n_observations&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;pland_04_deciduous_broadleaf&quot;, &quot;pland_05_mixed_forest&quot;, &quot;pland_12_cropland&quot;, &quot;pland_13_urban&quot;), obs_covs = c(&quot;time_observations_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;pland_04_deciduous_broadleaf&quot;, &quot;pland_05_mixed_forest&quot;)) 5.2.2 Spatial subsampling As discussed in Section 4.3, spatial subsampling of eBird observations reduces spatial bias. We’ll use the same hexagonal subsampling approach as in Chapter 4; however, here we’ll subsample at the level of sites rather than observations. For this example, we will sample one site per 5 km grid cell. Note, that because we included observer_id in the site definition, this subsampling process will only select one set of visits from one observer to one site, within each 5km cell. # generate hexagonal grid with ~ 5 km betweeen cells dggs &lt;- dgconstruct(spacing = 5) # get hexagonal cell id for each site occ_wide_cell &lt;- occ_wide %&gt;% mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum) # sample one site per grid cell occ_ss &lt;- occ_wide_cell %&gt;% group_by(cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() %&gt;% select(-cell) # calculate the percent decrease in the number of sites 1 - nrow(occ_ss) / nrow(occ_wide) This resulted in a 41% decrease in the number of sites. 5.2.3 unmarked object Finally, we’ll convert this data frame of observations into an unmarked object so we can start fitting occupancy models. occ_um &lt;- formatWide(occ_ss, type = &quot;unmarkedFrameOccu&quot;) summary(occ_um) #&gt; unmarkedFrame Object #&gt; #&gt; 584 sites #&gt; Maximum number of observations per site: 10 #&gt; Mean number of observations per site: 3.86 #&gt; Sites with at least one detection: 64 #&gt; #&gt; Tabulation of y observations: #&gt; 0 1 &lt;NA&gt; #&gt; 2125 128 3587 #&gt; #&gt; Site-level covariates: #&gt; n_observations latitude longitude pland_04_deciduous_broadleaf #&gt; Min. : 2.00 Min. :29.7 Min. :-91.4 Min. :0.000 #&gt; 1st Qu.: 2.00 1st Qu.:31.2 1st Qu.:-85.6 1st Qu.:0.000 #&gt; Median : 2.00 Median :33.1 Median :-81.4 Median :0.000 #&gt; Mean : 3.86 Mean :33.2 Mean :-82.0 Mean :0.052 #&gt; 3rd Qu.: 5.00 3rd Qu.:35.1 3rd Qu.:-78.3 3rd Qu.:0.000 #&gt; Max. :10.00 Max. :37.4 Max. :-75.5 Max. :1.000 #&gt; pland_05_mixed_forest pland_12_cropland pland_13_urban #&gt; Min. :0.000 Min. :0.000 Min. :0.000 #&gt; 1st Qu.:0.000 1st Qu.:0.000 1st Qu.:0.000 #&gt; Median :0.000 Median :0.000 Median :0.000 #&gt; Mean :0.047 Mean :0.025 Mean :0.159 #&gt; 3rd Qu.:0.000 3rd Qu.:0.000 3rd Qu.:0.203 #&gt; Max. :1.000 Max. :1.000 Max. :1.000 #&gt; #&gt; Observation-level covariates: #&gt; time_observations_started duration_minutes effort_distance_km number_observers #&gt; Min. : 0 Min. : 1 Min. :0 Min. :1 #&gt; 1st Qu.: 8 1st Qu.: 16 1st Qu.:0 1st Qu.:1 #&gt; Median :10 Median : 35 Median :0 Median :1 #&gt; Mean :12 Mean : 53 Mean :1 Mean :1 #&gt; 3rd Qu.:16 3rd Qu.: 70 3rd Qu.:1 3rd Qu.:1 #&gt; Max. :24 Max. :300 Max. :5 Max. :4 #&gt; NA&#39;s :3587 NA&#39;s :3587 NA&#39;s :3587 NA&#39;s :3587 #&gt; protocol_type pland_04_deciduous_broadleaf pland_05_mixed_forest #&gt; Length:5840 Min. :0 Min. :0 #&gt; Class :character 1st Qu.:0 1st Qu.:0 #&gt; Mode :character Median :0 Median :0 #&gt; Mean :0 Mean :0 #&gt; 3rd Qu.:0 3rd Qu.:0 #&gt; Max. :1 Max. :1 #&gt; NA&#39;s :3587 NA&#39;s :3587 5.3 Occupancy modeling Now that we’ve created a data frame with detection histories and covariates, we can use unmarked to fit a single-season occupancy model. As mentioned above, readers can discover more about occupancy models and the variety of modeling approaches in MacKenzie et al. (2017). Here, we simply fit a single-season occupancy model to our data using the occu() function, specifying the detection and occupancy covariates, respectively, via a double right-hand sided formula of the form ~ detection covariates ~ occupancy covariates. # fit model occ_model &lt;- occu(~ time_observations_started + duration_minutes + effort_distance_km + number_observers + protocol_type + pland_04_deciduous_broadleaf + pland_05_mixed_forest ~ pland_04_deciduous_broadleaf + pland_05_mixed_forest + pland_12_cropland + pland_13_urban, data = occ_um) # look at the regression coefficients from the model summary(occ_model) #&gt; #&gt; Call: #&gt; occu(formula = ~time_observations_started + duration_minutes + #&gt; effort_distance_km + number_observers + protocol_type + pland_04_deciduous_broadleaf + #&gt; pland_05_mixed_forest ~ pland_04_deciduous_broadleaf + pland_05_mixed_forest + #&gt; pland_12_cropland + pland_13_urban, data = occ_um) #&gt; #&gt; Occupancy (logit-scale): #&gt; Estimate SE z P(&gt;|z|) #&gt; (Intercept) -2.030 0.231 -8.773 1.74e-18 #&gt; pland_04_deciduous_broadleaf 6.742 1.958 3.442 5.77e-04 #&gt; pland_05_mixed_forest 0.889 0.789 1.126 2.60e-01 #&gt; pland_12_cropland -1.116 1.906 -0.586 5.58e-01 #&gt; pland_13_urban -2.011 0.968 -2.078 3.77e-02 #&gt; #&gt; Detection (logit-scale): #&gt; Estimate SE z P(&gt;|z|) #&gt; (Intercept) -1.24620 0.56367 -2.211 0.02704 #&gt; time_observations_started -0.00505 0.02993 -0.169 0.86592 #&gt; duration_minutes 0.00650 0.00317 2.053 0.04005 #&gt; effort_distance_km -0.25544 0.13388 -1.908 0.05639 #&gt; number_observers 0.16704 0.30811 0.542 0.58773 #&gt; protocol_typeTraveling 0.84136 0.38631 2.178 0.02941 #&gt; pland_04_deciduous_broadleaf -0.77822 0.53981 -1.442 0.14940 #&gt; pland_05_mixed_forest 2.87185 1.00784 2.849 0.00438 #&gt; #&gt; AIC: 691 #&gt; Number of sites: 584 #&gt; optim convergence code: 0 #&gt; optim iterations: 67 #&gt; Bootstrap iterations: 0 5.3.1 Assessment Although few goodness-of-fit tests exist for occupancy models, we demonstrate how to perform the MacKenzie and Bailey (2004) goodness-of-fit test. This approach calculates a Pearson’s chi-square fit statistic from the observed and expected frequencies of detection histories for a given model. For this example, we use the mb.gof.test() test function in the AICcmodavg package, which can handle occupancy models produced by the occu() function in unmarked. Note that to produce accurate results, this process requires simulating a large number of bootstrap samples, which can take a long time to run. To keep the execution times reasonable, we set nsim = 10 to simulate 10 samples for this example; however, when running this under regular circumstances, you should increase this to a much higer number of simulations (e.g., nsim = 1000). occ_gof &lt;- mb.gof.test(occ_model, nsim = 10, plot.hist = FALSE) # hide the chisq table to give simpler output occ_gof$chisq.table &lt;- NULL print(occ_gof) #&gt; #&gt; MacKenzie and Bailey goodness-of-fit for single-season occupancy model #&gt; #&gt; Chi-square statistic = 1555 #&gt; Number of bootstrap samples = 1000 #&gt; P-value = 0.621 #&gt; #&gt; Quantiles of bootstrapped statistics: #&gt; 0% 25% 50% 75% 100% #&gt; 537 1387 1715 2254 35800 #&gt; #&gt; Estimate of c-hat = 0.73 For this example, the probability of getting the calculated chi-square statistic under a null sampling distribution is indicated by the p-value of 0.621. As p &gt; 0.1 there is no reason to consider a lack of fit. We also get an estimate of the overdispersion parameter (c-hat) for the model, which is derived by dividing the observed chi-square statistic by the mean of the statistics obtained from simulation. In this example, c-hat = 0.73, which is very close to c-hat = 1, indicating that the variance is not greater than the mean, and that there is no evidence for overdispersion. Again, under regular circumstances we would want to run many more simulations, but based on this smaller run, the test statistics suggest that there is no evidence of lack of fit of this model to these data. For more details on this test, see MacKenzie and Bailey (2004). 5.3.2 Model selection So far, we have a single global model that includes all of the covariates we believe will influence the occupancy and detection probabilities. In general, we suggest that careful consideration be used when choosing the set of candidate models to be run and compared during model selection. In this example, we will use the dredge() function to generate a set of candidate models using different combinations of the covariates in the global model. Since we know from prior experience that the effort covariates are almost always important, we’ll lock these variables in, and consider a candidate set consisting of all possible combinations of the ecological covariates in the occupancy submodel. # get list of all possible terms, then subset to those we want to keep det_terms &lt;- getAllTerms(occ_model) %&gt;% # retain the detection submodel covariates discard(str_detect, pattern = &quot;psi&quot;) # dredge all possibe combinations of the occupancy covariates occ_dredge &lt;- dredge(occ_model, fixed = det_terms) # model comparison mc &lt;- select(occ_dredge, starts_with(&quot;psi(p&quot;), df, AICc, delta, weight) # shorten names for printing names(mc) &lt;- names(mc) %&gt;% str_extract(&quot;(?&lt;=psi\\\\(pland_[0-9]{2}_)[a-z_]+&quot;) %&gt;% coalesce(names(mc)) mc %&gt;% mutate_all(~ round(., 3)) %&gt;% knitr::kable() deciduous_broadleaf mixed_forest cropland urban df AICc delta weight 7.08 -2.09 11 689 0.000 0.366 6.88 0.935 -1.95 12 690 0.783 0.248 6.92 -1.271 -2.15 12 690 1.543 0.169 6.74 0.889 -1.116 -2.01 13 691 2.459 0.107 7.97 1.262 11 693 4.335 0.042 8.31 10 693 4.578 0.037 7.91 1.243 -0.598 12 695 6.301 0.016 8.23 -0.759 11 695 6.463 0.014 -3.06 10 717 28.051 0.000 -2.271 -3.14 11 717 28.324 0.000 0.961 -2.89 11 717 28.459 0.000 0.883 -2.098 -2.98 12 718 29.001 0.000 1.446 10 729 40.192 0.000 1.404 -1.446 11 730 41.535 0.000 9 731 41.922 0.000 -1.649 10 732 43.041 0.000 The corrected Akaike Information Criterion (AICc) measures the likelihood of each model to have generated the data we observed, adjusted for the number of parameters in the model. Lower values indicate models with a better fit to the data, penalizing for the added number of parameters. Delta is the difference in AICc values between the given model and the model that is most likely to have generated the data (i.e. the one with the lowest AICc), and is a relative measure conditional on the candidate set of models. Finally, the AIC weight is a transformation of delta that can be interpreted as the probability that the given model is the most likely one of the candidate models to have generated the data, and is also conditional on the candidate model set. The first four columns give the model coefficients for each of the habitat covariates in the occupancy submodel; missing values indicate that that covariate was not included in the given model. A quick look at the results of dredging reveals that for the Wood Thrush example there is not a clear single model, or even a small set of models, that are most likely to have generated our data. This is evident from the low AIC weight for the top model and the large number of models with moderate AIC weights. Given this, and the fact that all of our effects are linear and use the same family and link function, we’ll average across all models, weighted by AICc, to produce a model-averaged prediction. However, there may be scenarios in which there is a clear set of high performing models, in which case you can use the get.models() function to extract just these models prior to averaging. For the sake of efficiency, we’ll only average the top models, which we’ll define as those cumulatively comprising 95% of the weights. This will trivially impact the results since the models with lower support have very small weights and therefore contribute little to the weighted-average predictions. # select models with the most suport for model averaging occ_dredge_95 &lt;- get.models(occ_dredge, subset = cumsum(weight) &lt;= 0.95) # average models based on model weights occ_avg &lt;- model.avg(occ_dredge_95, fit = TRUE) # model coefficients t(occ_avg$coefficients) #&gt; full subset #&gt; psi(Int) -2.01638 -2.01638 #&gt; psi(pland_04_deciduous_broadleaf) 6.99654 6.99654 #&gt; psi(pland_13_urban) -1.95991 -2.05220 #&gt; p(Int) -1.26469 -1.26469 #&gt; p(duration_minutes) 0.00654 0.00654 #&gt; p(effort_distance_km) -0.25472 -0.25472 #&gt; p(number_observers) 0.16099 0.16099 #&gt; p(pland_04_deciduous_broadleaf) -0.77321 -0.77321 #&gt; p(pland_05_mixed_forest) 2.98243 2.98243 #&gt; p(protocol_typeTraveling) 0.84620 0.84620 #&gt; p(time_observations_started) -0.00557 -0.00557 #&gt; psi(pland_05_mixed_forest) 0.40717 0.95693 #&gt; psi(pland_12_cropland) -0.35917 -1.21116 5.3.3 Exploring detection model submodel A unique feature of occupancy models, is that we can investigate whether certain covariates influence detection separately from any influencing occurrence, which wasn’t possible using the machine learning approach in Chapter 4. Specifically, we have already seen that the habitat covariates influence occupancy, but we can assess how these covariates affect detection probability, conditional on a bird being present and potentially detected. We included deciduous broadleaf forest (pland_04) and mixed forest (pland_05) as detection covariates in the global model, and we can compare a set of models with and without these covariates to assess their importance. Here we’ll demonstrate how to manually define and compare a set of models rather than use the dredge() function as we did in the previous section. Let’s start by defining the candidate model set. To do so, we explicitly define a null detection model with no habitat covariates, then add in additional terms using update.formula(). # define a null detection model det_mod &lt;- ~ time_observations_started + duration_minutes + effort_distance_km + number_observers + protocol_type ~ pland_04_deciduous_broadleaf + pland_05_mixed_forest + pland_12_cropland + pland_13_urban # define and fit candidate models mods &lt;- list(det_mod_null = det_mod, det_mod_dec = update.formula(det_mod, ~ . + pland_04_deciduous_broadleaf ~ .), det_mod_mix = update.formula(det_mod, ~ . + pland_05_mixed_forest ~ .), global = update.formula(det_mod, ~ . + pland_04_deciduous_broadleaf + pland_05_mixed_forest ~ .)) %&gt;% map(occu, data = occ_um) Now we can perform model selection on this set to compare the different candidate models. mod_sel &lt;- fitList(fits = mods) %&gt;% modSel() mod_sel #&gt; nPars AIC delta AICwt cumltvWt #&gt; global 13 690.60 0.00000 0.478 0.48 #&gt; det_mod_mix 12 690.60 0.00057 0.478 0.96 #&gt; det_mod_dec 12 696.58 5.98599 0.024 0.98 #&gt; det_mod_null 11 696.99 6.39447 0.020 1.00 From these results, it’s clear that including these forest habitat covariates–especially deciduous broadleaf forest–leads to a marked improvement in model performance, as observed by the differences in AIC and the AIC weights. Let’s look at the coefficients from the global model for these covariates to see how they’re impacting detection and occupancy. coef(occ_model) %&gt;% enframe() %&gt;% filter(str_detect(name, &quot;pland_0&quot;)) #&gt; # A tibble: 4 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 psi(pland_04_deciduous_broadleaf) 6.74 #&gt; 2 psi(pland_05_mixed_forest) 0.889 #&gt; 3 p(pland_04_deciduous_broadleaf) -0.778 #&gt; 4 p(pland_05_mixed_forest) 2.87 The psi() coefficients are from the occupancy submodel and the p() coefficients are from the detection submodel. With this in mind, we see that increasing forest cover increases occupancy probability, since Wood Thrush are forest-associated species, but it decreases detection probability, since birds are harder to see and hear in dense forest. The ability to tease apart the differing effects that covariates have on detection and occupancy is one of the strengths of occupancy modeling. 5.4 Prediction In this section, we’ll estimate the distribution of Wood Thrush in BCR 27. Similar to Section 3.4, we’ll generate a prediction surface using the PLAND land cover covariates summarized on a regular grid of points across BCR 27. For this, we’ll use the predict() function to estimate occupancy probabilities, standard errors, and confidence intervals. When we use predict() on the output of get.models() it will make predictions for each of the selected models, then average the predictions using the AIC weights to produce the final prediction for each location. Recall that when we predicted encouter rate, we had to include effort variables in our prediction surface. We don’t need to do that here because the occupancy submodel doesn’t depend on the effort covariates; these only occur in the detection submodel. occ_pred &lt;- predict(occ_avg, newdata = as.data.frame(pred_surface), type = &quot;state&quot;) # add to prediction surface pred_occ &lt;- bind_cols(pred_surface, occ_prob = occ_pred$fit, occ_se = occ_pred$se.fit) %&gt;% select(latitude, longitude, occ_prob, occ_se) Next, we’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction surface raster template. r_pred &lt;- pred_occ %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = projection(r)) %&gt;% # rasterize rasterize(r) r_pred &lt;- r_pred[[c(&quot;occ_prob&quot;, &quot;occ_se&quot;)]] # save the raster tif_dir &lt;- &quot;output&quot; if (!dir.exists(tif_dir)) { dir.create(tif_dir) } writeRaster(r_pred[[&quot;occ_prob&quot;]], filename = file.path(tif_dir, &quot;occupancy-model_prob_woothr.tif&quot;), overwrite = TRUE) writeRaster(r_pred[[&quot;occ_se&quot;]], filename = file.path(tif_dir, &quot;occupancy-model_se_woothr.tif&quot;), overwrite = TRUE) Finally, we can map these predictions! # project predictions r_pred_proj &lt;- projectRaster(r_pred, crs = map_proj$proj4string, method = &quot;ngb&quot;) par(mfrow = c(2, 1)) for (nm in names(r_pred)) { r_plot &lt;- r_pred_proj[[nm]] par(mar = c(3.5, 0.25, 0.25, 0.25)) # set up plot area plot(bcr, col = NA, border = NA) plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) # occupancy probability or standard error if (nm == &quot;occ_prob&quot;) { title &lt;- &quot;Wood Thrush Occupancy Probability&quot; brks &lt;- seq(0, 1, length.out = 21) lbl_brks &lt;- seq(0, 1, length.out = 11) %&gt;% round(2) } else { title &lt;- &quot;Wood Thrush Occupancy Uncertainty (SE)&quot; mx &lt;- ceiling(1000 * cellStats(r_plot, max)) / 1000 brks &lt;- seq(0, mx, length.out = 21) lbl_brks &lt;- seq(0, mx, length.out = 11) %&gt;% round(2) } pal &lt;- abundance_palette(length(brks) - 1) plot(r_plot, col = pal, breaks = brks, maxpixels = ncell(r_plot), legend = FALSE, add = TRUE) # borders plot(bcr, border = &quot;#000000&quot;, col = NA, lwd = 1, add = TRUE) plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) plot(ne_country_lines, col = &quot;#ffffff&quot;, lwd = 1.5, add = TRUE) box() # legend par(new = TRUE, mar = c(0, 0, 0, 0)) image.plot(zlim = range(brks), legend.only = TRUE, breaks = brks, col = pal, smallplot = c(0.25, 0.75, 0.06, 0.09), horizontal = TRUE, axis.args = list(at = lbl_brks, labels = lbl_brks, fg = &quot;black&quot;, col.axis = &quot;black&quot;, cex.axis = 0.75, lwd.ticks = 0.5, padj = -1.5), legend.args = list(text = title, side = 3, col = &quot;black&quot;, cex = 1, line = 0)) } References "],
["abundance.html", "Chapter 6 Modeling Relative Abundance 6.1 Introduction 6.2 Data preparation 6.3 Exploratory data analysis 6.4 Abundance models 6.5 Asssessment 6.6 Prediction 6.7 Final thoughts", " Chapter 6 Modeling Relative Abundance 6.1 Introduction The previous two chapters focused on modeling occurrence. However, in addition to recording which species they observed, most eBirders also specify how many individuals of each species were observed. So, in this chapter, we’ll take advantage of these counts to model a relative measure of species abundance. To motivate this section, we will focus on the specific goal of estimating a map of relative abundance. This type of map would help us to identifyareas with higher or lower abundance. The metric we’ll use to estimate abundance is the expected number of individuals observed on a standardized eBird checklist. Like the encounter rate model, the abundance model we present in this section accounts for variation in detection rates but it does not directly estimate the absolute detection probability. For this reason, the estimates of abundance can only be interpreted as a measure of relative abundance; an index of the count of the individuals of the species present in the search area. To match the common terminology in the literature, we refer to this as an estimate of relative abundance. In some respects, the abundance model presented here is similar to the encounter rate model of Chapter 4. We start by spatiotemporally subsampling the training data to reduce the effects of spatial bias. In a departure from the methodology used for modelling encounter rate, we use semi-parametric models, fitting models of relative abundance using Generalized Additive Models (GAMs) to predict the count response. We have chosen to use GAMs because they can flexibly include many covariates while offering a choice of several error distributions suitable for count responses. It’s important to evaluate different count distributions because the distribution of eBird counts can vary strongly for different species, seasons, and regions, and the choice of distribution can have substantial impacts on model estimates. We’ll test three different distributions (zero-inflated Poisson, negative binomial, and Tweedie) and assess which of these fit the data best using cross validation. Finally, we’ll make predictions of relative abundance throughout BCR 27 and produce a map of these predictions. 6.2 Data preparation Let’s start by loading the necessary packages and data. If you created or downloaded the files needed to follow the analyses in the previous chapters, you may want to download the data package and unzip it to your project directory. Because we’re modeling abundance in this chapter, we’ll remove any records for which the observer reported that Wood Thrush was present, but didn’t report a count of the number of species (coded as ‘X’ records in the eBird database). library(lubridate) library(sf) library(raster) library(dggridR) library(pdp) library(edarf) library(mgcv) library(fitdistrplus) library(viridis) library(fields) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select map &lt;- purrr::map projection &lt;- raster::projection # set random number seed to insure fully repeatable results set.seed(1) # setup output directory for saved results if (!dir.exists(&quot;output&quot;)) { dir.create(&quot;output&quot;) } # ebird data ebird &lt;- read_csv(&quot;data/ebd_woothr_june_bcr27_zf.csv&quot;) %&gt;% mutate(protocol_type = factor(protocol_type, levels = c(&quot;Stationary&quot; , &quot;Traveling&quot;))) %&gt;% # remove observations with no count filter(!is.na(observation_count)) # modis habitat covariates habitat &lt;- read_csv(&quot;data/pland-elev_location-year.csv&quot;) %&gt;% mutate(year = as.integer(year)) # combine ebird and habitat data ebird_habitat &lt;- inner_join(ebird, habitat, by = c(&quot;locality_id&quot;, &quot;year&quot;)) # prediction surface pred_surface &lt;- read_csv(&quot;data/pland-elev_prediction-surface.csv&quot;) # latest year of landcover data max_lc_year &lt;- pred_surface$year[1] r &lt;- raster(&quot;data/prediction-surface.tif&quot;) # load gis data for making maps map_proj &lt;- st_crs(102003) ne_land &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_land&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() bcr &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;bcr&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_country_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_country_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_state_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_state_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() 6.2.1 Spatiotemporal subsampling As discussed in Section 4.3, spatiotemporal subsampling detection and non-detection observations reduces both spatial and temporal bias and the class imbalance. We’ll use exactly the same hexagonal subsampling approach as in Chapter 4. # generate hexagonal grid with ~ 5 km betweeen cells dggs &lt;- dgconstruct(spacing = 5) # get hexagonal cell id and week number for each checklist checklist_cell &lt;- ebird_habitat %&gt;% mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum, week = week(observation_date)) # sample one checklist per grid cell per week # sample detection/non-detection independently ebird_ss &lt;- checklist_cell %&gt;% group_by(species_observed, year, week, cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() %&gt;% select(-cell, -week) 6.2.2 Test-train split Before we fit the abundance models, we randomly split the data into 80% of checklists for training and 20% for testing. We’ll hold this 20% aside when we fit the model, then use it as an independent data set with which to test the predictive performance of the model. Here we select a random 20% of the data, but there are a variety of strategies to select data for testing that may be appropriate in different situations. At this stage, we’ll also retain only the variables that we’ll use as covariates in the models. In particular, we’ll use the full suite of effort covariates and the same four habitat covariates we used in Chapter 5. Deciduous broadleaf forest and mixed forest are known Wood Thrush breeding habitat, and these thrushes are known to avoid croplands and urban. The specific set of habit covariates you use will be specific for your species and should be informed by a priori ecological knowledge of the species. See Section 3.1 for a list of the habitat covariates available in this data set. hab_covs &lt;- c(&quot;pland_04_deciduous_broadleaf&quot;, &quot;pland_05_mixed_forest&quot;, &quot;pland_12_cropland&quot;, &quot;pland_13_urban&quot;) ebird_split &lt;- ebird_ss %&gt;% # select only the columns to be used in the model select(observation_count, # effort covariates day_of_year, time_observations_started, duration_minutes, effort_distance_km, number_observers, protocol_type, # habitat covariates hab_covs) # split 80/20 ebird_split &lt;- ebird_split %&gt;% split(if_else(runif(nrow(.)) &lt;= 0.8, &quot;train&quot;, &quot;test&quot;)) map_int(ebird_split, nrow) #&gt; test train #&gt; 3941 16142 6.3 Exploratory data analysis Before we embark on modelling the counts, we’ll start by examining the distribution of the count data. This will help give us an idea of which distributions may be appropriate for modeling the counts of this species. Looking at the distribution of counts with and without the zeros can help reveal evidence of zero-inflation. eBird data often have a very high number of zero counts, since even common bird species are not seen on every checklist. Large numbers of zeros can also arise when the study extent borders on the species’ range boundary. p &lt;- par(mfrow = c(1, 2)) # counts with zeros hist(ebird_ss$observation_count, main = &quot;Histogram of counts&quot;, xlab = &quot;Observed count&quot;) # counts without zeros pos_counts &lt;- keep(ebird_ss$observation_count, ~ . &gt; 0) hist(pos_counts, main = &quot;Histogram of counts &gt; 0&quot;, xlab = &quot;Observed non-zero count&quot;) par(p) The plot that includes zeros (left) shows an extremely zero-inflated and skewed distribution, due to the large number of zero-counts (checklists with no Wood Thrush detections). For the counts only (right), the data still show a highly skewed distribution, with lots of checklists with low numbers and only a few checklists with more than five Wood Thrush. These plots and conclusions are only indicative—they provide some information about what we can expect for distributions at the next step. Overall, for Wood Thrush we can conclude that the counts are highly skewed with many zero observations. 6.4 Abundance models Prior to fitting our GAM models, let’s construct the model formula that we’ll use when we call the fitting function. In a GLM where the relationship between a covariate and the response is linear, whereas a GAM allows smooth non-linear relationships. The degree of variation (wiggliness) allowed in each covariate relationship is controlled by the degrees of freedom. The k parameter determines the maximum wiggliness of the smooth, with higher values allowing for more potential wiggliness. GAMs use a data-driven approach to adaptively reduce the wiggliness (degrees of freedom) based on the training data. There are a wide variety of different types of smooth relationships. We use a different different type of smooth for the checklist start time relationship. We want to joins the ends of the relationship, i.e. 0 and 24 are both midnight, so the smooth should be the same value at these points. To accomplish this we’ll use a cubic cyclic spline (bs = &quot;cc&quot;). The following code builds the GAM model formula, using only the covariates chosen in the previous section. Using this approach allows the formula to be automatically re-built in order to match any changes that one might make in the habitat types represented in the ebird_split data frame. If, for example, you decide to explore using a different set of covariates when modeling the abundance for a different species then the model formula will be built appropriately for your species. # gam parameters # degrees of freedom for smoothing k &lt;- 5 # degrees of freedom for cyclic time of day smooth k_time &lt;- 7 # continuous predictors # hold out time to treat seperately since it&#39;s cyclic continuous_covs &lt;- ebird_split$train %&gt;% select(-observation_count, -protocol_type, -time_observations_started) %&gt;% names() # create model formula for predictors gam_formula_rhs &lt;- str_glue(&quot;s({var}, k = {k})&quot;, var = continuous_covs, k = k) %&gt;% str_flatten(collapse = &quot; + &quot;) %&gt;% str_glue(&quot; ~ &quot;, ., &quot; + protocol_type + &quot;, &quot;s(time_observations_started, bs = \\&quot;cc\\&quot;, k = {k})&quot;, k = k_time) %&gt;% as.formula() # model formula including response gam_formula &lt;- update.formula(observation_count ~ ., gam_formula_rhs) gam_formula #&gt; observation_count ~ s(day_of_year, k = 5) + s(duration_minutes, #&gt; k = 5) + s(effort_distance_km, k = 5) + s(number_observers, #&gt; k = 5) + s(pland_04_deciduous_broadleaf, k = 5) + s(pland_05_mixed_forest, #&gt; k = 5) + s(pland_12_cropland, k = 5) + s(pland_13_urban, #&gt; k = 5) + protocol_type + s(time_observations_started, bs = &quot;cc&quot;, #&gt; k = 7) Alternatively, the formula can be defined manually: gam_formula &lt;- observation_count ~ s(day_of_year, k = 5) + s(duration_minutes, k = 5) + s(effort_distance_km, k = 5) + s(number_observers, k = 5) + s(pland_04_deciduous_broadleaf, k = 5) + s(pland_05_mixed_forest, k = 5) + s(pland_12_cropland, k = 5) + s(pland_13_urban, k = 5) + protocol_type + s(time_observations_started, bs = &quot;cc&quot;, k = 7) Now we’ll use this formula to fit GAM models, testing the following three count response distributions: Zero-inflated Poisson: This distribution effectively fits the data in two parts: (1) a binomial model that determines the variables associated with species presence and (2) a Poisson count model for those places with species presence, that determines the variables associated with species count. This is an effective distribution when there are a large number of zero counts in the data and the positive counts approximate a Poisson distribution. Negative binomial: The negative binomial distribution is related to the Poisson distribution. However, the variance can be considerably larger in the negative binomial distribution. This distribution is appropriate for data when the variance of the counts is much larger than the mean of the counts — a situation called over-dispersion that is very common in ecological count data. Tweedie distribution: This is a very flexible distribution that encompasses a wide variety of shapes, including those with extremely high variance relative to the mean and extreme over-dispersion. The Tweedie distribution fit in GAM spans a range of distributions from the Poisson to the gamma, so it is a very flexible option. # explicitly specify where the knots should occur for time_observations_started # this ensures that the cyclic spline joins the variable at midnight # this won&#39;t happen by default if there are no data near midnight time_knots &lt;- list(time_observations_started = seq(0, 24, length.out = k_time)) # zero-inflated poisson m_ziplss &lt;- gam(list(gam_formula, # count model gam_formula[-2]), # presence model data = ebird_split$test, family = &quot;ziplss&quot;, knots = time_knots) # negative binomial m_nb &lt;- gam(gam_formula, data = ebird_split$train, family = &quot;nb&quot;, knots = time_knots) # tweedie distribution m_tw &lt;- gam(gam_formula, data = ebird_split$train, family = &quot;tw&quot;, knots = time_knots) 6.5 Asssessment Here we assess the predictive performance among the three models. Recall from Section 4.4.2 that this involves making predictions for the test dataset, which wasn’t used to fit the models, then assessing how well these predictions relate to the actual observed counts in the test dataset. Care needs to be taken when making predictions from the zero-inflated Poisson model since it has two components: probability of presence and expected count given presence. As a result, the predict() function returns a two column matrix with the count and probability respectively, both on the scale of the link functions. So, we need to back-transform these values, then multiply them to get the expected counts. There may be situations in which you want to summarise these two values in different ways. obs_count &lt;- select(ebird_split$test, obs = observation_count) # presence probability is on the complimentary log-log scale # we can get the inverse link function with inv_link &lt;- binomial(link = &quot;cloglog&quot;)$linkinv # combine ziplss presence and count predictions m_ziplss_pred &lt;- predict(m_ziplss, ebird_split$test, type = &quot;link&quot;) %&gt;% as.data.frame() %&gt;% transmute(family = &quot;Zero-inflated Poisson&quot;, pred = inv_link(V2) * exp(V1)) %&gt;% bind_cols(obs_count) m_nb_pred &lt;- predict(m_nb, ebird_split$test, type = &quot;response&quot;) %&gt;% tibble(family = &quot;Negative Binomial&quot;, pred = .) %&gt;% bind_cols(obs_count) m_tw_pred &lt;- predict(m_tw, ebird_split$test, type = &quot;response&quot;) %&gt;% tibble(family = &quot;Tweedie&quot;, pred = .) %&gt;% bind_cols(obs_count) # combine predictions from all three models test_pred &lt;- bind_rows(m_ziplss_pred, m_nb_pred, m_tw_pred) %&gt;% mutate(family = as_factor(family)) 6.5.1 Predictive performance: Ranking Assessing the fit of the models depends considerably on the goals of the model and hence on the most important aspects of the model fit for the intended use. For our motivating use-case, estimating a map of relative abundance for the purpose of identifying areas with higher or lower abundance, we will evaluate the models based on Spearman’s Rank Correlation. This will assess how well the model estimated the ranking of the sites from highest to lowest abundance. # spearman’s rank correlation test_pred %&gt;% group_by(family) %&gt;% summarise(rank_cor = cor.test(obs, pred, method = &quot;spearman&quot;, exact = FALSE)$estimate) %&gt;% ungroup() family rank_cor Zero-inflated Poisson 0.205 Negative Binomial 0.225 Tweedie 0.226 In terms of ranking, the zero-inflated Poisson performs worst, having the lowest rank correlation. However, all the models are fairly similar and none have a strong correlation between predicted and observed counts. 6.5.2 Predictive performance: Magnitude For other applications, it may also be important to generate accurate estimates of the magnitude of the relative abundance. For example, when identifying migration hot-spots it may be important to estimate high counts accurately to distinguish between multiple stop over sites. Or, if the goal is to generate an index of the total population size, it may be more important to generate accurate estimates of small to medium counts, and place less emphasis on the relatively rare high counts. We begin the assessment of the quality of the magnitude of abundance estimates by looking at a plot of predicted and observerved counts from the test set. For illustrative purposes, lets assume for our situation that underestimated abundances are more problematic than overestimated abundances. We’ll highlight in red on these plots the regions where the predictions are underestimated by more than an order of magnitude. We’ll also overlay the line \\(y = x\\), which separates overestimates (above the line) from underestimates (below the line), and a blue smoothed fit showing the general trend through all the data. # plot predicted vs. observed ticks &lt;- c(0, 1, 10, 100, 1000) mx &lt;- round(max(test_pred$obs)) ggplot(test_pred) + aes(x = log10(obs + 1), y = log10(pred + 1)) + geom_jitter(alpha = 0.2, height = 0) + # y = x line geom_abline(slope = 1, intercept = 0, alpha = 0.5) + # area where counts off by a factor of 10 geom_area(data = tibble(x = log10(seq(0, mx - 1) + 1), y = log10(seq(0, mx - 1) / 10 + 1)), mapping = aes(x = x, y = y), fill = &quot;red&quot;, alpha = 0.2) + # loess fit geom_smooth(method = &quot;loess&quot;, method.args = list(span = 2 / 3, degree = 1)) + scale_x_continuous(breaks = log10(ticks + 1), labels = ticks) + scale_y_continuous(breaks = log10(ticks + 1), labels = ticks) + labs(x = &quot;Observed count&quot;, y = &quot;Predicted count&quot;) + facet_wrap(~ family, nrow = 1) Overall, we see that most of the counts are underestimated, since most points and the blue line are below the gray \\(y = x\\) line. We also see that Wood Thrush was not observed on most of these checklists. Although most predictions for these checklists are also zero (since the blue line reaches 0 predicted abundance at 0 actual abundance), there are still many cases with predicted Wood Thrush occurrence where none were observed. It’s hard to say how much of problem this kind of “overestimation” is since detection is known to be imperfect. Finally, we see that there are also a large number of observations that are underestimated by an order of magnitude or more. How many of these cases are there? test_pred %&gt;% group_by(family) %&gt;% summarize(n = sum(obs / pred &gt; 10), pct = mean(obs / pred &gt; 10)) family n pct Zero-inflated Poisson 233 0.059 Negative Binomial 145 0.037 Tweedie 142 0.036 Based on this metric, it appears that the zero-inflated Poisson model is performing worst in terms of these “large” underestimates. Finally, to get an overall sense of the quality of the magnitude of the predicted counts we will compute the test set mean absolute deviation (MAD), a robust statistic that describes the average deviation between observation and prediction. Note, we are not using the mean squared error because it will place more emphasis on errors when counts are high. # mean absolute deviation test_pred %&gt;% group_by(family) %&gt;% summarise(mad = mean(abs(obs - pred), na.rm = TRUE)) %&gt;% ungroup() family mad Zero-inflated Poisson 0.142 Negative Binomial 0.181 Tweedie 0.181 In terms of magnitude, the zero-inflated Poisson model performs the worst across most metrics: it has the largest number of problematic errors, but the lowest MAD. The negative binomial and Tweedie models are almost identical in terms of performace. Either model could be chosen; however, the negative binomial is a simpler model, with fewer parameters to estimate, so it seems reasonable to pick it to make our predictions. Depending on your focal species and region, as well as the particular goals of your analysis, some aspects of model fit will be more important than others, so it’s important to consider your assessment criteria and make sure they match your application. 6.5.3 Assessing covariate effects We recommend also assessing the fitted covariate effects to see whether they show biologically plausible relationships between covariates and species counts. Splines can sometimes overfit, notably when sample sizes are very large, and in this cases it is appropriate to reduce the degrees of freedom. Calling plot() on the fitted GAM objects produces plots of the smooth functions for each of the separate predictors, which gives us a sense of the effect of each predictor on the count response. Unfortunately, for large numbers of predictors, plot() will produce a cramped, unreadable plot. Instead, we write a function to capture the data behind the plotting function, then plot them using ggplot2 instead. # ggplot function plot_gam &lt;- function(m, title = NULL, ziplss = c(&quot;presence&quot;, &quot;abundance&quot;)) { # capture plot tmp &lt;- tempfile() png(tmp) p &lt;- plot(m, pages = 1) dev.off() unlink(tmp) # drop addition models in ziplss if (m$family$family == &quot;ziplss&quot;) { is_presence &lt;- map_lgl(p, ~ str_detect(.$ylab, &quot;^s\\\\.1&quot;)) if (ziplss == &quot;presence&quot;) { p &lt;- p[is_presence] } else { p &lt;- p[!is_presence] } } # extract data p_df &lt;- map_df(p, ~ tibble(cov = rep(.$xlab, length(.$x)), x = .$x, fit = .$fit, se = .$se)) # plot g &lt;- ggplot(p_df) + aes(x = x, y = fit, ymin = fit - se, ymax = fit + se) + geom_ribbon(fill = &quot;grey80&quot;) + geom_line(col = &quot;blue&quot;) + facet_wrap(~ cov, scales = &quot;free_x&quot;) + labs(x = NULL, y = &quot;Smooth function&quot;, title = title) print(g) invisible(p_df) } plot_gam(m_nb, title = &quot;Negative Binomial GAM&quot;) If these relationships seem too wiggly to be biologically realistic, you should reduce the degrees of freedom for the smooth until a biologically feasible relationship is achieved. Only use these graphs to assess the wiggliness of the predicted effects, as the values plotted on the y-axes of these graphs are standardized values. In this case, the relationships appear to be reasonable for the negative binomial model, so we will use it for prediction below. # set negative binomial model to be used for predictions pred_model &lt;- m_nb 6.6 Prediction Now that we’ve selected the negative binomial GAM, we can use this model to map Wood Thrush relative abundance in BCR 27! In Section 3.4, we created a prediction surface consisting of the PLAND habitat covariates summarized on a regular grid of points across BCR 27. In this section, we’ll make predictions of relative abundance at these points. However, first we need to bring effort variables into this prediction surface. We’ll use a standard eBird checklist: a 1 km, 1 hour traveling count at the peak time of day for detecting this species. To determine this peak time, we’ll predict abundance and its 95% confidence limits at a series of times throughout the day, then pick the time at which the lower confidence limit is at its maximum. By using the lower confidence limits, we select a time that we are confident has high detectability and thus avoid potentially unrealistic predictions from times of day for which few or no data existed. # create a dataframe of covariates with a range of start times seq_tod &lt;- seq(0, 24, length.out = 300) tod_df &lt;- ebird_split$train %&gt;% # find average pland habitat covariates select(starts_with(&quot;pland&quot;)) %&gt;% summarize_all(mean, na.rm = TRUE) %&gt;% ungroup() %&gt;% # use standard checklist mutate(day_of_year = yday(ymd(str_glue(&quot;{max_lc_year}-06-15&quot;))), duration_minutes = 60, effort_distance_km = 1, number_observers = 1, protocol_type = &quot;Traveling&quot;) %&gt;% cbind(time_observations_started = seq_tod) # predict at different start times pred_tod &lt;- predict(pred_model, newdata = tod_df, type = &quot;link&quot;, se.fit = TRUE) %&gt;% as_tibble() %&gt;% # calculate backtransformed confidence limits transmute(time_observations_started = seq_tod, pred = pred_model$family$linkinv(fit), pred_lcl = pred_model$family$linkinv(fit - 1.96 * se.fit), pred_ucl = pred_model$family$linkinv(fit + 1.96 * se.fit)) # find optimal time of day t_peak &lt;- pred_tod$time_observations_started[which.max(pred_tod$pred_lcl)] # plot the partial dependence plot ggplot(pred_tod) + aes(x = time_observations_started, y = pred, ymin = pred_lcl, ymax = pred_ucl) + geom_ribbon(fill = &quot;grey80&quot;, alpha = 0.5) + geom_line() + geom_vline(xintercept = t_peak, color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;Hours since midnight&quot;, y = &quot;Predicted relative abundance&quot;, title = &quot;Effect of observation start time on Wood Thrush reporting&quot;, subtitle = &quot;Peak detectability shown as dashed blue line&quot;) So, the peak time of day for detecting Wood Thrush is around 5:13 AM. Let’s generate the prediction surface and make predictions at all the points. In addition to relative abundance, we’ll estimate standard error and 95% confidence limits. # add effort covariates to prediction surface pred_surface_eff &lt;- pred_surface %&gt;% mutate(day_of_year = yday(ymd(str_glue(&quot;{max_lc_year}-06-15&quot;))), time_observations_started = t_peak, duration_minutes = 60, effort_distance_km = 1, number_observers = 1, protocol_type = &quot;Traveling&quot;) # predict pred &lt;- predict(pred_model, newdata = pred_surface_eff, type = &quot;link&quot;, se.fit = TRUE) %&gt;% as_tibble() %&gt;% # calculate confidence limits and back transform transmute(abd = pred_model$family$linkinv(fit), abd_se = pred_model$family$linkinv(se.fit), abd_lcl = pred_model$family$linkinv(fit - 1.96 * se.fit), abd_ucl = pred_model$family$linkinv(fit + 1.96 * se.fit)) %&gt;% # add to prediction surface bind_cols(pred_surface_eff, .) %&gt;% select(latitude, longitude, abd, abd_se, abd_lcl, abd_ucl) Next, we’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction surface raster template. r_pred &lt;- pred %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% select(abd, abd_se) %&gt;% st_transform(crs = projection(r)) %&gt;% # rasterize rasterize(r) r_pred &lt;- r_pred[[-1]] # save the rasters tif_dir &lt;- &quot;output&quot; if (!dir.exists(tif_dir)) { dir.create(tif_dir) } writeRaster(r_pred[[&quot;abd&quot;]], filename = file.path(tif_dir, &quot;abundance-model_abd_woothr.tif&quot;), overwrite = TRUE) writeRaster(r_pred[[&quot;abd_se&quot;]], filename = file.path(tif_dir, &quot;abundance-model_se_woothr.tif&quot;), overwrite = TRUE) Finally, let’s make a map! For the relative abundance map, we’ll treat very small values of relative abundance as zero. The values shown on this map are the expected number of Wood Thrush seen by an average eBirder conducting a 1 hour, 1 km checklist for which counting started at about 5:13 AM. As detectability is not perfect, we expect true Wood Thrush abundance to be higher than these values, but without estimating the detection rate directly it’s difficult to say how much higher. Also remember at this point that there was a lot of variation in eBird counts that wasn’t explained by the model. So this map shows the best estimate of relative abundance, given the model and variables selected for modeling. # any expected abundances below this threshold are set to zero zero_threshold &lt;- 0.05 # project predictions r_pred_proj &lt;- projectRaster(r_pred, crs = map_proj$proj4string, method = &quot;ngb&quot;) par(mfrow = c(2, 1)) for (nm in names(r_pred)) { r_plot &lt;- r_pred_proj[[nm]] par(mar = c(3.5, 0.25, 0.25, 0.25)) # set up plot area plot(bcr, col = NA, border = NA) plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) # modified plasma palette plasma_rev &lt;- rev(plasma(25, end = 0.9)) gray_int &lt;- colorRampPalette(c(&quot;#dddddd&quot;, plasma_rev[1])) pal &lt;- c(gray_int(4)[2], plasma_rev) # abundance vs. se if (nm == &quot;abd&quot;) { title &lt;- &quot;Wood Thrush Relative Abundance&quot; # set very low values to zero r_plot[r_plot &lt;= zero_threshold] &lt;- NA # log transform r_plot &lt;- log10(r_plot) # breaks and legend mx &lt;- ceiling(100 * cellStats(r_plot, max)) / 100 mn &lt;- floor(100 * cellStats(r_plot, min)) / 100 brks &lt;- seq(mn, mx, length.out = length(pal) + 1) lbl_brks &lt;- sort(c(-2:2, mn, mx)) lbls &lt;- round(10^lbl_brks, 2) } else { title &lt;- &quot;Wood Thrush Abundance Uncertainty (SE)&quot; # breaks and legend mx &lt;- ceiling(1000 * cellStats(r_plot, max)) / 1000 mn &lt;- floor(1000 * cellStats(r_plot, min)) / 1000 brks &lt;- seq(mn, mx, length.out = length(pal) + 1) lbl_brks &lt;- seq(mn, mx, length.out = 5) lbls &lt;- round(lbl_brks, 2) } # abundance plot(r_plot, col = pal, breaks = brks, maxpixels = ncell(r_plot), legend = FALSE, add = TRUE) # borders plot(bcr, border = &quot;#000000&quot;, col = NA, lwd = 1, add = TRUE) plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) plot(ne_country_lines, col = &quot;#ffffff&quot;, lwd = 1.5, add = TRUE) box() # legend par(new = TRUE, mar = c(0, 0, 0, 0)) image.plot(zlim = range(brks), legend.only = TRUE, col = pal, smallplot = c(0.25, 0.75, 0.06, 0.09), horizontal = TRUE, axis.args = list(at = lbl_brks, labels = lbls, fg = &quot;black&quot;, col.axis = &quot;black&quot;, cex.axis = 0.75, lwd.ticks = 0.5, padj = -1.5), legend.args = list(text = title, side = 3, col = &quot;black&quot;, cex = 1, line = 0)) } 6.7 Final thoughts GAMs are complex and powerful models, but assessing their fit and adapting them accordingly is sometimes more of an art than a science. We recommend people using these functions for their own models consult with a statistician or someone with experience with GAMs. A good reference for GAMs is (Wood 2017). References "],
["references.html", "References", " References "]
]
